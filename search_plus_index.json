{"./":{"url":"./","title":"项目简介","keywords":"","body":"PHP PHP-Interview-QA php-interview-2018 PHPerInterviewGuide php-engineer-interview-questions interview php-be-intervie PHP面试题及答案 mysql优化面试题 PHP8新特性之JIT简介 PHP8新特性盘点 PHP7新特性 CGI、FastCGI和PHP-FPM关系图解 PHP7为什么比5快 消息队列 消息队列高频问题 2021最新 RabbitMQ面试题精选 缓存 redis中跳表原理&实现 码上一波 Redis 面试题，面试跳槽不要慌! 2021一波最新 Redis 面试题 2021年最新版 68道Redis面试题 史上最全的50个Redis面试题及答案 2021年Redis面试题（持续更新） Linux 统计nginx日志里访问次数最多的前十个IP awk '{print $1}' /var/log/nginx/access.log | sort | uniq -c | sort -nr -k1 | head -n 10 常见算法 数据结构与算法系列目录 树常见面试题 红黑树常见面试问题整理 红黑树之原理和算法详细介绍 B树，B+树，红黑树 数据库常见面试题 腾讯、阿里面试题 了解B+树吗？ 删除链表倒数第N个节点 value = $value; } } /** * @param $node 头节点 * @param $n 倒数第几个节点 * 思路：要删除倒数第n个节点，我们就要找到其前面一个节点，也就是倒数第n+1个节点，找到这个节点就可以进行删除 * 定义两个指针，p和cur，cur指针向前走，走了n+1步之后，p指针开始走，当cur指针走到链表结尾的时候，p指针刚好走到倒数第n+1个节点处 */ function delete($node, $n = 2) { if(empty($node)) { return $node; } $header = new node(0); $header->next = $node;//头节点不能丢失(有可能是删除头节点) $cur = $node; $p = $node;//n的个数比节点数大时，删除第一个节点 //cur指针先移动n步 for($i=0; $inext; } //找到倒数第n+1个节点 while($cur->next != null) { $cur = $cur->next; $p = $p->next; } //删除 $p->next = $p->next->next; return $header->next; } //创建链表可封装成一个方法 $A = new Node(1); $B = new Node(2); $C = new Node(3); $D = new Node(4); $E = new Node(5); $A->next = $B; $B->next = $C; $C->next = $D; $D->next = $E; echo \"\"; print_r(delete($A, 2)); ?> 二分查找 #二分查找 function binarySearch(Array $arr, $target) { $low = 0; $high = count($arr) - 1; while($low $target) $high = $mid - 1; #重元素比目标小,查找右部 if($arr[$mid] 冒泡排序 $demo_array = array(23,15,43,25,54,2,6,82,11,5,21,32,65); // 第一层for循环可以理解为从数组中键为0开始循环到最后一个 for ($i=0;$i $demo_array[$j]) { $tmp = $demo_array[$i]; // 这里的tmp是临时变量 $demo_array[$i] = $demo_array[$j]; // 第一次更换位置 $demo_array[$j] = $tmp; // 完成位置互换 } } } 快速排序 function quick_sort($arr){ //先判断是否需要继续进行 $length = count($arr); if($length $arr[$i]) { //放入左边数组 $left_array[] = $arr[$i]; } else { //放入右边 $right_array[] = $arr[$i]; } } //再分别对左边和右边的数组进行相同的排序处理方式递归调用这个函数 $left_array = quick_sort($left_array); $right_array = quick_sort($right_array); //合并 return array_merge($left_array, array($base_num), $right_array);; } 常见问题 laravel执行流程 1、载入Composer的自动加载文件，自动加载的真正实现是通过/vendor/autoload.php实现的，代码如下 2、加载/bootstrap/app.php文件，实例化服务容器，存在$app 3、向服务容器里绑定了三个服务：HTTP、Console、Excepiton 4、make方法取出http，存到$request变量中（$request变量贯穿始终） 5、按照app配置文件顺序register所有的服务提供者 6、按照注册顺序执行所有服务提供者的boot方法 7、将请求发送到route 8、执行中间件 9、发送请求 10、返回响应 Redis SDS 分配规则: 当字符串长度小于 1M 时，扩容都是加倍现有的空间，如果超过 1M，扩容时一次只会多扩 1M 的空间 typedef定义的 char*,C语言printf 能直接打印 struct sdshdr { unsigned int len; //buf中已经使用的长度 unsigned int free; //buf中未使用的长度 char buf[]; //柔性数组buf }; 在Redis 3.2 版本中，对数据结构做出了修改，针对不同的长度范围定义了不同的结构 len表示sds当前sds的长度(单位是字节)，不包括'0'终止符，通过len直接获取字符串长度，不需要扫一遍string，这就是上文说的封装sds的理由之一； alloc表示当前为sds分配的大小(单位是字节)(3.2以前的版本用的free是表示还剩free字节可用空间)，不包括'0'终止符； flags表示当前sdshdr的类型，声明为char 一共有1个字节(8位)，仅用低三位就可以表示所有5种sdshdr类型(详见上文代码注释): Redis跳表的实现,延迟队列的实现 Redis跳表的实现是由一个双向链表和分成结构实现的, 分成的机制是有抛硬币决定的,连续抛出正面的次数决定了他的层数, 节点中只保存了索引书和指针浪费的内存可以忽略不计 延时队列使用有序集合实现 score 存到期时间key为消息内容, 并使用zrangebyscore(0 - 当前时间,limit 1)进行消费 大批量导入数据（百万级别以上） mysql load data 拆分文件 多开 InnoDB Buffer 关闭binlog 改 myisam PHP insert合并 事务 有序 扛得住可以多开 tcp和udp的区别 TCP/IP(链路层,网络层,传输层,应用层) UDP(无连接,单播-多播-广播,面向报文,不可靠) UDP无连接(不可靠,支持一对一,一对多,多对一和多对多交互通信,面向报文,首部开销小，仅8字节),适用于实时应用（IP电话、视频会议、直播等） TCP面向连接(可靠,只能是一对一通信,面向字节流,首部最小20字节，最大60字节),适用于要求可靠传输的应用，例如文件传输 击穿缓存，直接访问数据库，导致服务不可用怎么办 在缓存失效的时候（判断拿出来的值为空），不是立即去load db，而是先使用缓存工具的某些带成功操作返回值的操作（比如Redis的SETNX或者Memcache的ADD）去set一个mutex key，当操作返回成功时，再进行load db的操作并回设缓存, 否则，就重试整个get缓存的方法 事前：Redis 高可用，主从+哨兵，Redis cluster，避免全盘崩溃。 事中：本地 ehcache 缓存 + Hystrix 限流+降级，避免MySQL被打死。 事后：Redis 持久化 RDB+AOF，一旦重启，自动从磁盘上加载数据，快速恢复缓存数据。 给定一个大文件（内存放不下），如何排序 PHP可以使用迭代器,实现流程: 拆分成多个小文件 -> 对单个小文件排序 -> 在从各个小文件读取指定数据(可顺序,倒叙)进行排序 -> 合并输出 php-fpm原理 php-fpm是一种master（主）/worker（子）多进程架构，与nginx设计风格有点类似。master进程主要负责CGI及PHP环境初始化、事件监听、子进程状态等等, worker进程负责处理php请求。 master只是负责管理工作，并不是很多人认为的把客户端发来的请求分给worker进程处理，而是由worker进程负责客户端的请求监听和处理, master只负责管理worker，如重启，重新加载配置文件，并不会派发请求。 worker是同步阻塞,必须等这次处理完，才会处理其它请求,并发性能不行. 杀掉 master 正常工作 MySQL主从同步不一致的原因 原因 人为原因导致从库与主库数据不一致（从库写入） 主从复制过程中，主库异常宕机 设置了ignore/do/rewrite等replication等规则 binlog非row格式 异步复制本身不保证，半同步存在提交读的问题，增强半同步起来比较完美。 但对于异常重启（Replication Crash Safe），从库写数据（GTID）的防范，还需要策略来保证。 从库中断很久，binlog应用不连续，监控并及时修复主从 从库启用了诸如存储过程，从库禁用存储过程等 数据库大小版本/分支版本导致数据不一致？，主从版本统一 备份的时候没有指定参数 例如mysqldump --master-data=2 等 主从sql_mode 不一致 一主二从环境，二从的server id一致 MySQL自增列 主从不一致 主从信息保存在文件里面，文件本身的刷新是非事务的，导致从库重启后开始执行点大于实际执行点 采用5.6的after_commit方式半同步，主库当机可能会引起主从不一致，要看binlog是否传到了从库 启用增强半同步了（5.7的after_sync方式），但是从库延迟超时自动切换成异步复制 预防和解决的方案 配置 redolog&binlog 及时写入磁盘 设置从库库为只读模式 可以使用5.7增强半同步避免数据丢失等 binlog row格式 必须引定期的数据校验机制 当使用延迟复制的时候，此时主从数据也是不一致的（计划内），但在切换中，不要把延迟从提升为主库哦~ mha在主从切换的过程中，因主库系统宕机，可能造成主从不一致（mha本身机制导致这个问题） mysql事务的底层实现 mysql事务的底层实现 事务具有ACID四个特性。也即：原子性，一致性，隔离性，持久性 原子性的实现: 这个事务，要么成功commit，要么失败rollback。其实更简单的来说，原子性需要保证，事务发生了异常，能够rollback。 原子性的核心，就是一个可回滚的操作。 undo log(回滚日志）undo log事务中，用于存放数据被修改时操作相反的逻辑操作。 持久性的实现: 一个事务一旦提交，它对数据库中数据的改变就应该是永久性的。但是事务回滚，这个数据是不会改变的。所以在持久性的体现主要在事务提交。在MySQL实现持久性，主要是通过redo log保证的 redo log(重做日志) redo log包括两部分，重做日志缓冲（redo log buffer）和重做日志文件（redo log file），前者是易失的缓存，后者是持久化的文件。 当事务提交时，必须先将事务的所有日志写入日志文件进行持久化 隔离性的实现: 隔离性，就是根据容忍程度，定义了四个隔离级别，解决三个问题。 索引的原理，B-tree和B+tree的区别 索引的原理: 通过不断的缩小想要获得数据的范围来筛选出最终想要的结果，同时把随机的事件变成顺序的事件，也就是我们总是通过同一种查找方式来锁定数据。 MyISAM索引文件和数据文件是分离的，索引文件仅保存数据记录的地址。而在InnoDB中，表数据文件本身就是按B+Tree组织的一个索引结构，这棵树的叶节点data域保存了完整的数据记录。这个索引的key是数据表的主键，因此InnoDB表数据文件本身就是主索引。 B-tree中非叶子节点可以存值；但是B+tree非叶子节点不可以存值，只能存key，值只存在叶子节点中。 B-tree中叶子节点没有用指针连接起来；而B+tree中的叶子节点用指针连接起来，所以B+tree的查询很快，当定位到叶子节点后，只需要遍历叶子节点即可。B+树相比于B树能够更加方便的遍历。 B+树简单的说就是变成了一个索引一样的东西。 B+的搜索与B-树也基本相同，区别是B+树只有达到叶子结点才命中（B-树可以在非叶子结点命中），B+树的性能相当于是给叶子节点做一次二分查找。 B+树的查找算法：当B+树进行查找的时候，你首先一定需要记住，就是B+树的非叶子节点中并不储存节点，只存一个键值方便后续的操作，所以非叶子节点就是索引部分，所有的叶子节点是在同一层上，包含了全部的关键值和对应数据所在的地址指针。这样其实，进行 B+树的查找的时候，只需要在叶子节点中进行查找就可以了。 mvcc的原理 MVCC是通过在每行记录后面保存两个隐藏的列来实现的。这两个列，一个保存了行的创建时间，一个保存行的过期时间（或删除时间）。当然存储的并不是实际的时间值，而是系统版本号（system version number)。每开始一个新的事务，系统版本号都会自动递增。事务开始时刻的系统版本号会作为事务的版本号，用来和查询到的每行记录的版本号进行比较。 前缀索引 对于列的值较长，比如BLOB、TEXT、VARCHAR，就必须建立前缀索引，即将值的前一部分作为索引。这样既可以节约空间，又可以提高查询效率。但无法使用前缀索引做 ORDER BY 和 GROUP BY，也无法使用前缀索引做覆盖扫描 Mysql回表原理 MySQL innodb的主键索引是簇集索引，也就是索引的叶子节点存的是整个单条记录的所有字段值，不是主键索引的就是非簇集索引，非簇集索引的叶子节点存的是主键字段的值。回表是什么意思？就是你执行一条sql语句，需要从两个b+索引中去取数据。举个例子： 表tbl有a,b,c三个字段，其中a是主键，b上建了索引，然后编写sql语句 SELECT * FROM tbl WHERE a=1 这样不会产生回表，因为所有的数据在a的索引树中均能找到 SELECT * FROM tbl WHERE b=1 这样就会产生回表，因为where条件是b字段，那么会去b的索引树里查找数据，但b的索引里面只有a,b两个字段的值，没有c，那么这个查询为了取到c字段，就要取出主键a的值，然后去a的索引树去找c字段的数据。查了两个索引树，这就叫回表。 索引覆盖就是查这个索引能查到你所需要的所有数据，不需要去另外的数据结构去查。其实就是不用回表。 怎么避免？不是必须的字段就不要出现在SELECT里面。或者b,c建联合索引。但具体情况要具体分析，索引字段多了，存储和插入数据时的消耗会更大。这是个平衡问题。 常见协议层 OSI分层(7层) 物理层 -> 数据链路层 -> 网络层 -> 运输层 -> 会话层 -> 表示层 -> 应用层 五层协议(是OSI和TCP/IP的综合) 物理层 -> 数据链路层 -> 网络层 -> 运输层 -> 应用层 TCP/IP(4层) 网络接口层 -> 网际层 -> 运输层 -> 应用层 TCP三次握手 为什么要三次握手: 为了防止已失效的连接请求报文段突然又传送到了服务端，因而产生错误, 三次握手保证了数据的正确性，避免了因网络问题带来的问题。同时防止server端的一直等待，浪费资源。 1. 客户端发送一个TCP的SYN=1的包(SYN=1, seq=客户端序列号) 客户端进入SYN_SEND状态 2. 服务器发回ACK确认包(SYN=1, ACK=1, seq=服务端序列号, ACKnum(确认序号)=客户的ISN加1) 服务器端进入SYN_RCVD状态 3. 客户端再次发送ACK确认包(ACK=1，seq=客服端的ISN+1 ACKnum(确认序号)=服务端的ISN加1) 客户端进入ESTABLISHED(已确认)状态,服务端收到到后进入ESTABLISHED(已确认)状态 TCP四次挥手 为什么要四次挥手: 确保数据能够完成传输, 当收到对方的FIN报文通知时，它仅仅表示对方没有数据发送给你了；但未必你所有的数据都全部发送给对方了,你可能还需要发送一些数据给对方之后，再发送FIN报文给对方来表示你同意现在可以关闭连接了，所以它这里的ACK报文和FIN报文多数情况下都是分开发送的 1. 客服端发送FIN=1标志位置为1的包(FIN=1，seq=x) 客户端进入FIN_WAIT_1状态 2. 服务器端发送一个ACK确认包(ACK=1，ACKnum=x+1) 服务器端进入CLOSE_WAIT状态 (客户端接收到这个确认包之后，进入FIN_WAIT_2状态) 3. 服务器端发送一个结束连接请求(FIN=1，seq=y) 服务器端进入LAST_ACK状态(等待来自客户端的最后一个ACK) 4. 客户端接收到后发送一个ACK确认包，并进入TIME_WAIT状态(可能出现的要求重传的ACK包) 等待了某个固定时间 进入CLOSED状态 Cookie&Session区别 存储位置不同 (cookie放在客服端,session在服务端) 存储容量不同 (cookie大小受浏览器限制,对于session来说并没有上限) 隐私策略不同 (cookie对客户端是可见的,session存储在服务器上) Http报文格式 请求 请求行：请求方法 + URL + 协议/版本号 请求头部：浏览器告知给服务端的属性信息 空行 请求主体：请求相关的数据信息（参数等） 响应 响应行：协议/版本号 + 状态码 + 描述信息 响应头部：服务端告知浏览器的属性信息 空行 响应主体：响应数据(html文件等) 一个完整的HTTP&HTTPS请求 http: 域名解析 -> 发起TCP的3次握手 -> 建立TCP连接后发起http请求 -> 服务器响应http请求 -> TCP连接关闭 -> 浏览器得到html代码 -> 渲染 https(HTTPS跟HTTP一样，只不过增加了SSL): 验证服务器端 -> 允许客户端和服务器端选择加密算法和密码，确保双方都支持 -> 验证客户端(可选) -> 使用公钥加密技术来生成共享加密数据 -> 创建一个加密的SSL连接 -> 基于该 SSL 连接传递 HTTP 请求 对称加密和非对称加密的区别 对称加密: 是最快速、最简单的一种加密方式, 加密与解密用的是同样的密钥 非对称加密: 1. A要向B发送信息，A和B都要产生一对用于加密和解密的公钥和私钥。 2. A的私钥保密，A的公钥告诉B；B的私钥保密，B的公钥告诉A。 3. A要给B发送信息时，A用B的公钥加密信息，因为A知道B的公钥。 4. A将这个消息发给B（已经用B的公钥加密消息）。 5. B收到这个消息后，B用自己的私钥解密A的消息，其他所有收到这个报文的人都无法解密，因为只有B才有B的私钥。 6. 反过来，B向A发送消息也是一样。 常见状态码 参考 Http状态码 400：bad requst - 通常是请求语法问题 403：forbidden - 认证授权未通过 404：not found - 资源不存在 500：Internal server error - 不可预期的错误 502：gateway bad - 收到后端无效响应 503：service unavaible - 由于临时的服务器维护或者过载 504：gateway timeout - 服务器尝试执行请求时未能及时收到响应 499：认为是不安全的连接，主动拒绝了客户端的连接 GET和POST方法区别 数据存储位置: Get方法放在URL中；Post方法放到body中 数据大小限制: 浏览器对URL长度有限制 安全性: 用户名和密码等参数出现在URL上存在安全隐患 数据类型: GET只允许ASCII字符,POST没有限制 HTTP1.1和HTTP2.0的区别 多路复用：连接共享机制让一个连接让可以有多个request请求，服务端通过request请求id区分后让不同的服务端进行响应，让原本的串行的请求-响应模式变为并行，提高传输处理效率 二进制格式：让原本基于文本格式的解析方法变为二进制格式的解析方法，以此来提高解析报文的健壮性 header压缩：通过缓存头部字段来减少传输大小和避免重复发送header字段问题 服务端推送：把客户端所需要的资源伴随着index.html一起发送到客户端，省去了客户端重复请求的步骤，以此提高性能 长连接：同一个TCP连接可以承载多个http请求和响应；而多路复用指的是同一个http连接可以共享 秒杀&超卖问题 参考 https://blog.csdn.net/zhoudaxia/article/details/38067003 秒杀系统的设计与实现 分两个步骤解决: 1. 前端 扩容(加机器，这是最简单的方法，通过增加前端机器来抗峰值) 静态化(将页面上的所有可以静态的元素全部静态化，并尽量减少动态元素。通过CDN来抗峰值。) 限流(可以通过限制 IP,按钮,用户 ID 等来限制单人短时间内发起的请求数量) 拒绝请求(随机拒绝部分请求来保护整体的可用性,可以使用 nginx 进行限流) 2. 后端 业务分离(将秒杀业务系统和其他业务分离，单独放在高配服务器上，可以集中资源对访问请求抗压) 使用缓存(热点数据都从缓存获得，尽可能减小数据库的访问压力) 库存前置(放入内存中异步同步到数据库,可以使用队列或Redis事务进行减库存操作) 使用队列(商品放入队列中,来一个人取一个过程中有操作失败,可重新返回队列) 分流(部署多台机器,通过负载均衡共同处理客户端请求，分散压力。) 异步(如发短信、更新数据库等，从而缓解服务器峰值压力。) "},"数据库/索引使用说明&注意事项.html":{"url":"数据库/索引使用说明&注意事项.html","title":"索引使用说明&注意事项","keywords":"","body":"数据库创建索引能够大大提高系统的性能。 通过创建唯一性的索引，可以保证数据库表中每一行数据的唯一性。 可以大大加快数据的检索速度，这也使创建索引的最主要的原因。 可以加速表和表之间的连接，特别是在实现数据的参考完整性方面特别有意义。 在使用分组和排序子句进行数据检索时，同样可以显著的减少查询中查询中分组和排序的时间。 通过使用索引，可以在查询的过程中，使用优化隐藏器，提高系统的性能。 增加索引也有许多不利的方面。 创建索引和维护索引需要消耗时间，这种时间随着数量的增加而增加。 索引需要占物理空间，除了数据表占据数据空间之外，每一个索引还要占一定的物理空间，如果要建立聚簇索引，那么需要额空间就会更大。 当对表中的数据进行增加，删除和修改的时候，索引也要动态的维护，这样就降低了数据的维护速度。 应该对如下的列建立索引 在作为主键的列上，强制该列的唯一性和组织表中数据的排列结构。 在经常用在连接的列上，这些列主要是一些外键，可以加快连接的速度。 在经常需要根据范围进行搜索的列上创建索引，因为索引已经排序，其指定的范围是连续的。 在经常需要排序的列上创建索引，因为索引已经排序，这样查询可以利用索引的排序，加快排序查询时间。 在经常使用在where子句中的列上面创建索引，加快条件的判断速度。 有些列不应该创建索引 在查询中很少使用或者作为参考的列不应该创建索引。 对于那些只有很少数据值的列也不应该增加索引（比如性别，结果集的数据行占了表中数据行的很大比例，即需要在表中搜索的数据行的比例很大。增加索引，并不能明显加快检索速度）。 对于那些定义为text，image和bit数据类型的列不应该增加索引。这是因为，这些列的数据量要么相当大，要么取值很少。 当修改性能远远大于检索性能时，不应该创建索引，因为修改性能和检索性能是矛盾的。 创建索引的方法 直接创建和间接创建（在表中定义主键约束或者唯一性约束时，同时也创建了索引）。 索引的特征 唯一性索引和复合索引。唯一性索引保证在索引列中的全部数据是唯一的，不会包含冗余数据。复合索引就是一个索引创建在两个列或者多个列上。可以减少一在一个表中所创建的索引数量。 "},"数据库/InnoDB&MyIsam的区别.html":{"url":"数据库/InnoDB&MyIsam的区别.html","title":"InnoDB&MyIsam的区别","keywords":"","body":" InnoDB支持事物，外键等高级的数据库功能，MyISAM不支持。需要注意的是，InnDB行级锁也不是绝对的，例如mysql执行一个未定范围的sql时，也还是会锁表，例如sql中like的使用 效率，明显MyISAM在插入数据的表现是InnoDB所远远不及的，在删改查，随着InnoDB的优化，差距渐渐变小 行数查询，InnoDB不保存行数，也就是select的时候，要扫描全表，MyISAM只需读取保存的行数即可，这也是MyISAM查询速度快的一个因素。 锁的支持。**MyISAM只支持表锁。InnoDB支持表锁、行锁 行锁大幅度提高了多用户并发操作的新能。但是InnoDB的行锁，只是在WHERE的主键是有效的，非主键的WHERE都会锁全表的 索引，InnoDB会自动创建Auto_Increment类型字段的索引，一般习惯应用于主键，即主键索引（只包含该字段），而MyISAM可以和其他字段创建联合索引。除此之外，MyISAM还支持全文索引（FULLTEXT_INDEX），压缩索引,InnoDB不支持(5.7以后的InnoDB支持全文索引了) 备注：MyISAM的索引和数据是分开的，并且索引是有压缩的，内存使用率就对应提高了不少。能加载更多索引，而Innodb是索引和数据是紧密捆绑的，没有使用压缩从而会造成Innodb比MyISAM体积庞大不小。InnoDB存储引擎被完全与MySQL服务器整合，InnoDB存储引擎为在主内存中缓存数据和索引而维持它自己的缓冲池。InnoDB存储它的表＆索引在一个表空间中，表空间可以包含数个文件（或原始磁盘分区）。这与MyISAM表不同，比如在MyISAM表中每个表被存在分离的文件中。InnoDB 表可以是任何尺寸，即使在文件尺寸被限制为2GB的操作系统上。 服务器数据备份。InnoDB必须导出SQL来备份，LOAD TABLE FROM MASTER操作对InnoDB是不起作用的，(解决方法是首先把InnoDB表改成MyISAM表，导入数据后再改成InnoDB表，但是对于使用的额外的InnoDB特性(例如外键)的表不适用。) 备注：而且MyISAM应对错误编码导致的数据恢复速度快。MyISAM的数据是以文件的形式存储，所以在跨平台的数据转移中会很方便。在备份和恢复时可单独针对某个表进行操作。InnoDB是拷贝数据文件、备份 binlog，或者用 mysqldump，支持灾难恢复（仅需几分钟），MyISAM不支持，遇到数据崩溃，基本上很难恢复，所以要经常进行数据备份。 MyISAM索引结构 MyISAM索引用的B+ tree来储存数据，MyISAM索引的指针指向的是键值的地址，地址存储的是数据。B+Tree的数据域存储的内容为实际数据的地址，也就是说它的索引和实际的数据是分开的，只不过是用索引指向了实际的数据，这种索引就是所谓的(非聚集索引) 因此，过程为： MyISAM中索引检索的算法为首先按照B+Tree搜索算法搜索索引，如果指定的Key存在，则取出其data域的值，然后以data域的值为地址，根据data域的值去读取相应数据记录。 InnoDB索引结构 用的也是B+Treee索引结构。Innodb的索引文件本身就是数据文件，即B+Tree的数据域存储的就是实际的数据，这种索引就是(聚集索引)。这个索引的key就是数据表的主键，因此InnoDB表数据文件本身就是主索引。 InnoDB的辅助索引数据域存储的也是相应记录主键的值而不是地址，所以当以辅助索引查找时，会先根据辅助索引找到主键，再根据主键索引找到实际的数据。所以Innodb不建议使用过长的主键，否则会使辅助索引变得过大。 与MyISAM索引的不同是InnoDB的辅助索引data域存储相应记录主键的值而不是地址。换句话说，InnoDB的所有辅助索引都引用主键作为data域。 叶节点包含了完整的数据记录。这种索引叫做聚集索引。因为InnoDB的数据文件本身要按主键聚集，所以InnoDB要求表必须有主键（MyISAM可以没有），如果没有显式指定，则MySQL系统会自动选择一个可以唯一标识数据记录的列作为主键，如果不存在这种列，则MySQL自动为InnoDB表生成一个隐含字段作为主键，这个字段长度为6个字节，类型为长整形。 建议使用自增的字段作为主键，这样B+Tree的每一个结点都会被顺序的填满，而不会频繁的分裂调整，会有效的提升插入数据的效率。 过程为：将主键组织到一棵B+树中，而行数据就储存在叶子节点上，若使用”where id = 13”这样的条件查找主键，则按照B+树的检索算法即可查找到对应的叶节点，之后获得行数据。若对Name列进行条件搜索，则需要两个步骤：第一步在辅助索引B+树中检索Name，到达其叶子节点获取对应的主键。第二步使用主键在主索引B+树种再执行一次B+树检索操作，最终到达叶子节点即可获取整行数据。 InnoDB为什么推荐使用自增ID作为主键 自增ID可以保证每次插入时B+索引是从右边扩展的，可以避免B+树和频繁合并和分裂（对比使用UUID）。如果使用字符串主键和随机主键，会使得数据随机插入，效率比较差。 innodb引擎的4大特性 插入缓冲（insert buffer),二次写(double write),自适应哈希索引(ahi),预读(read ahead) 如何选择? 是否要支持事务，如果要请选择innodb，如果不需要可以考虑MyISAM； 如果表中绝大多数都只是读查询，可以考虑MyISAM，如果既有读也有写，请使用InnoDB。 系统奔溃后，MyISAM恢复起来更困难，能否接受； MySQL5.5版本开始Innodb已经成为Mysql的默认引擎(之前是MyISAM)，说明其优势是有目共睹的，如果你不知道用什么，那就用InnoDB，至少不会差。 为什么使用索引会快? 索引就相当于一本书的目录，通过目录来找书中的某一页，确实是很快的，如果没有目录，就需要一页一页的去翻书了，大大降低了效率 当没有可用的索引时只能走全表扫描,即把主键索引上的叶子节点从头到尾都扫描一遍，然后每扫描到一行把字段 x 的值拿出来再比对一下，筛选出满足条件的记录，这个查询是非常低效的。 当有可以使用索引 x 时,该查询会先到二级索引 x 这个 B+ 树上，快速找到满足要求的叶子节点，而这里的叶子节点上只保存了主键的值，所以还需要通过获得的主键 ID 值再回到主键索引上查出所有字段的值 参考深入理解MySQL索引原理和实现——为什么索引可以加速查询？ "},"数据库/取第一天登录&第二天没有登录的用户.html":{"url":"数据库/取第一天登录&第二天没有登录的用户.html","title":"取第一天登录&第二天没有登录的用户","keywords":"","body":" 在 user_log 表中取出第一天登录第二天没有登录的用户ID id time 1 2019-06-01 1 2019-06-02 2 2019-06-01 3 2019-06-01 3 2019-06-02 4 2019-06-01 select id from user_log u1 where u1.time=\"2019-06-01\" and not exists(select 1 from user_login u2 where u1.id = u2.id and u2.time = '2019-06-02') "},"数据库/Mysql的索引类型.html":{"url":"数据库/Mysql的索引类型.html","title":"Mysql的索引类型","keywords":"","body":"B-Tree索引 最常见的索引类型，基于B-Tree数据结构。B-Tree的基本思想是，所有值（被索引的列）都是排过序的，每个叶节点到跟节点距离相等。所以B-Tree适合用来查找某一范围内的数据，而且可以直接支持数据排序（ORDER BY）。但是当索引多列时，列的顺序特别重要，需要格外注意。InnoDB和MyISAM都支持B-Tree索引。InnoDB用的是一个变种B+Tree，而MyISAM为了节省空间对索引进行了压缩，从而牺牲了性能。 Hash索引 [!TIP|label:HASH索引说明] 基于hash表。所以这种索引只支持精确查找，不支持范围查找，不支持排序。这意味着范围查找或ORDER BY都要依赖server层的额外工作。目前只有Memory引擎支持显式的hash索引（但是它的hash是nonunique的，冲突太多时也会影响查找性能）。Memory引擎默认的索引类型即是Hash索引，虽然它也支持B-Tree索引 哈希索引只包含哈希值和行指针，而不存储字段值，所以不能使用索引中的值来避免读取行（即不能使用哈希索引来做覆盖索引扫描），不过，访问内存中的行的速度很快（因为memory引擎的数据都保存在内存里），所以大部分情况下这一点对性能的影响并不明显。 哈希索引数据并不是按照索引列的值顺序存储的，所以也就无法用于排序 哈希索引也不支持部分索引列匹配查找，因为哈希索引始终是使用索引的全部列值内容来计算哈希值的。如：数据列（a,b）上建立哈希索引，如果只查询数据列a，则无法使用该索引。 哈希索引只支持等值比较查询，如：=,in(),(注意，<>和是不同的操作)，不支持任何范围查询（必须给定具体的where条件值来计算hash值，所以不支持范围查询）。 访问哈希索引的数据非常快，除非有很多哈希冲突，当出现哈希冲突的时候，存储引擎必须遍历链表中所有的行指针，逐行进行比较，直到找到所有符合条件的行。 如果哈希冲突很多的话，一些索引维护操作的代价也很高，如：如果在某个选择性很低的列上建立哈希索引（即很多重复值的列），那么当从表中删除一行时，存储引擎需要遍历对应哈希值的链表中的每一行，找到并删除对应的引用，冲突越多，代价越大。 Spatial (R-Tree)（空间）索引 只有MyISAM引擎支持，并且支持的不好。可以忽略。 Full-text索引 主要用来查找文本中的关键字，而不是直接与索引中的值相比较。Full-text索引跟其它索引大不相同，它更像是一个搜索引擎，而不是简单的WHERE语句的参数匹配。你可以对某列分别进行full-text索引和B-Tree索引，两者互不冲突。Full-text索引配合MATCH AGAINST操作使用，而不是一般的WHERE语句加LIKE。 参考 MySQL 的索引理解 btree与hash索引的适用场景和限制 MySQL有哪些索引类型 "},"数据库/Mysql聚集索引&非聚集索引.html":{"url":"数据库/Mysql聚集索引&非聚集索引.html","title":"Mysql聚集索引&非聚集索引","keywords":"","body":"聚集索引 [!TIP|label:说明] 聚集索引表记录的排列顺序与索引的排列顺序一致 非聚集索引 [!TIP|label:说明] 非聚集索引指定了表中记录的逻辑顺序，但记录的物理顺序和索引的顺序不一致 区别 聚集索引一个表只能有一个，而非聚集索引一个表可以存在多个。(因为这个聚簇索引的顺序就决定了数据的顺序) 聚集索引存储记录是物理上连续存在，而非聚集索引是逻辑上的连续，物理存储并不连续。 聚集索引查询数据速度快，插入数据速度慢(时间花费在“物理存储的排序”上，也就是首先要找到位置然后插入);非聚集索引反之。 说明 聚集索引表记录的排列顺序与索引的排列顺序一致，优点是查询速度快，因为一旦具有第一个索引值的纪录被找到，具有连续索引值的记录也一定物理的紧跟其后。 聚集索引的缺点是对表进行修改速度较慢，这是为了保持表中的记录的物理顺序与索引的顺序一致，而把记录插入到数据页的相应位置，必须在数据页中进行数据重排，降低了执行速度。插入数据时速度要慢(时间花费在“物理存储的排序”上，也就是首先要找到位置然后插入)。 非聚集索引指定了表中记录的逻辑顺序，但记录的物理顺序和索引的顺序不一致，聚集索引和非聚集索引都采用了B+树的结构，但非聚集索引的叶子层并不与实际的数据页相重叠，而采用叶子层包含一个指向表中的记录在数据页中的指针的方式。非聚集索引比聚集索引层次多，添加记录不会引起数据顺序的重组。 索引是通过二叉树的数据结构来描述的，我们可以这么理解聚簇索引：索引的叶节点就是数据节点。而非聚簇索引的叶节点仍然是索引节点，只不过有一个指针指向对应的数据块 "},"数据库/Mysql事物隔离级别.html":{"url":"数据库/Mysql事物隔离级别.html","title":"Mysql事物隔离级别","keywords":"","body":"事务的基本要素 原子性（Atomicity）：事务开始后所有操作，要么全部做完，要么全部不做，不可能停滞在中间环节。事务执行过程中出错，会回滚到事务开始前的状态，所有的操作就像没有发生一样。也就是说事务是一个不可分割的整体，就像化学中学过的原子，是物质构成的基本单位。 一致性（Consistency）：事务开始前和结束后，数据库的完整性约束没有被破坏 。比如A向B转账，不可能A扣了钱，B却没收到。 隔离性（Isolation）：同一时间，只允许一个事务请求同一数据，不同的事务之间彼此没有任何干扰。比如A正在从一张银行卡中取钱，在A取钱的过程结束前，B不能向这张卡转账。 持久性（Durability）：事务完成后，事务对数据库的所有更新将被保存到数据库，不能回滚。 事务的并发问题 不可重复读的和幻读很容易混淆，不可重复读侧重于修改，幻读侧重于新增或删除。解决不可重复读的问题只需锁住满足条件的行，解决幻读需要锁表 脏读：事务A读取了事务B更新的数据，然后B回滚操作，那么A读取到的数据是脏数据 不可重复读：事务 A 多次读取同一数据，事务 B 在事务A多次读取的过程中，对数据作了更新并提交，导致事务A多次读取同一数据时，结果 不一致。 幻读：系统管理员A将数据库中所有学生的成绩从具体分数改为ABCDE等级，但是系统管理员B就在这个时候插入了一条具体分数的记录，当系统管理员A改结束后发现还有一条记录没有改过来，就好像发生了幻觉一样，这就叫幻读。 MySQL事务隔离级别 mysql默认的事务隔离级别为 可重复读(repeatable-read) 事务隔离级别 脏读 不可重复读 幻读 读未提交（read-uncommitted） 是 是 是 读已提交（read-committed） 否 是 是 可重复读（repeatable-read） 否 否 是 串行化（serializable） 否 否 否 Mysql锁说明 共享锁(S锁) 用于只读操作(SELECT)，锁定共享的资源。共享锁不会阻止其他用户读，但是阻止其他的用户写和修改。 更新锁(U锁) 用于可更新的资源中。防止当多个会话在读取、锁定以及随后可能进行的资源更新时发生常见形式的死锁。 独占锁(X锁，也叫排他锁) 一次只能有一个独占锁用在一个资源上，并且阻止其他所有的锁包括共享缩。写是独占锁，可以有效的防止“脏读”。 事物隔离实现说明 读未提交 如果一个事务已经开始写数据，则另外一个数据则不允许同时进行写操作，但允许其他事务读此行数据。该隔离级别可以通过“排他写锁”实现。 读已提交 读取数据的事务允许其他事务继续访问该行数据，但是未提交的写事务将会禁止其他事务访问该行。可以通过“瞬间共享读锁”和“排他写锁”实现。 可重复读 读取数据的事务将会禁止写事务（但允许读事务），写事务则禁止任何其他事务。可以通过“共享读锁”和“排他写锁”实现。 串行化 读加共享锁，写加排他锁，读写互斥。 参考 MySQL的四种事务隔离级别 面试官：谈谈Mysql事务隔离级别？ mysql/mariadb知识点总结（21）：事务隔离级别 (事务总结之三) "},"数据库/mysql锁机制详解.html":{"url":"数据库/mysql锁机制详解.html","title":"mysql锁机制详解","keywords":"","body":"按锁的粒度划分 表锁：意向锁(IS锁、IX锁)、自增锁； 行锁：记录锁、间隙锁、临键锁、插入意向锁； 共享/排它锁(Shared and Exclusive Locks) 共享锁（Share Locks，记为S锁），读取数据时加S锁 (共享锁之间不互斥，简记为：读读可以并行) 排他锁（eXclusive Locks，记为X锁），修改数据时加X锁 (排他锁与任何锁互斥，简记为：写读，写写不可以并行) 排他锁，一旦写数据的任务没有完成，数据是不能被其他任务读取的，这对并发度有较大的影响。对应到数据库，可以理解为，写事务没有提交，读相关数据的select也会被阻塞，这里的select是指加了锁的，普通的select仍然可以读到数据(快照读)。 意向锁(Intention Locks) InnoDB为了支持多粒度锁机制(multiple granularity locking)，即允许行级锁与表级锁共存，而引入了意向锁(intention locks)。意向锁是指，未来的某个时刻，事务可能要加共享/排它锁了，先提前声明一个意向。 意向锁是一个表级别的锁(table-level locking)； 意向锁又分为： 意向共享锁(intention shared lock, IS)，它预示着，事务有意向对表中的某些行加共享S锁； 意向排它锁(intention exclusive lock, IX)，它预示着，事务有意向对表中的某些行加排它X锁； 记录锁(Record Locks) 记录锁，它封锁索引记录 间隙锁(Gap Locks) 间隙锁的主要目的，就是为了防止其他事务在间隔中插入数据，以导致“不可重复读”。如果把事务的隔离级别降级为读提交(Read Committed, RC)，间隙锁则会自动失效。 间隙锁，它封锁索引记录中的间隔，或者第一条索引记录之前的范围，又或者最后一条索引记录之后的范围 临键锁(Next-key Locks) 临键锁，是记录锁与间隙锁的组合，它的封锁范围，既包含索引记录，又包含索引区间。 默认情况下，innodb使用next-key locks来锁定记录。但当查询的索引含有唯一属性的时候，Next-Key Lock 会进行优化，将其降级为Record Lock，即仅锁住索引本身，不是范围。 插入意向锁(Insert Intention Locks) 对已有数据行的修改与删除，必须加强互斥锁(X锁)，那么对于数据的插入，是否还需要加这么强的锁，来实施互斥呢？插入意向锁，孕育而生。 插入意向锁，是间隙锁(Gap Locks)的一种（所以，也是实施在索引上的），它是专门针对insert操作的。多个事务，在同一个索引，同一个范围区间插入记录时，如果插入的位置不冲突，不会阻塞彼此。 自增锁(Auto-inc Locks) 自增锁是一种特殊的表级别锁（table-level lock），专门针对事务插入AUTO_INCREMENT类型的列。最简单的情况，如果一个事务正在往表中插入记录，所有其他事务的插入必须等待，以便第一个事务插入的行，是连续的主键值。 mysql什么时候会由行锁升级为表锁 innodb行锁机制是对索引进行加锁，如果索引失效那么升级为表所 如果没有使用索引，那么升级为表锁 想要在已被其他事务用排他锁占用的资源上面加锁，即意向共享锁，那么会行锁会升级为表锁 如何进行锁表 共享锁(S) SELECT*FROM table_nameWHERE... LOCK IN SHARE MODE 排他锁(X) SELECT*FROMtable_nameWHERE...FOR UPDATE 参考 mysql锁机制详解 MySQL锁机制与用法分析 MySQL InnoDB锁机制全面解析分享 乐观锁与悲观锁——解决并发问题 MySQL学习笔记（四）悲观锁与乐观锁 "},"数据库/Mysql主从同步原理.html":{"url":"数据库/Mysql主从同步原理.html","title":"Mysql主从同步原理","keywords":"","body":"MySQL 主从复制原理 [!TIP|label:说明] MySQL主从复制涉及到三个线程，一个运行在主节点（log dump thread），其余两个(I/O thread, SQL thread)运行在从节点,当主节点有多个从节点时,对于每一个主从连接,主节点会为每一个当前连接的从节点建一个binary log dump 进程 主节点 binary log dump 线程 当从节点连接主节点时，主节点会创建一个log dump 线程，用于发送bin-log的内容。在读取bin-log中的操作时，此线程会对主节点上的bin-log加锁，当读取完成，甚至在发动给从节点之前，锁会被释放。 从节点I/O线程 当从节点上执行start slave命令之后，从节点会创建一个I/O线程用来连接主节点，请求主库中更新的bin-log。I/O线程接收到主节点binlog dump 进程发来的更新之后，保存在本地relay-log中。 从节点SQL线程 SQL线程负责读取relay log中的内容，解析成具体的操作并执行，最终保证主从数据的一致性。 流程 数据库有个bin-log二进制文件，记录了所有sql语句。 binlog输出线程:每当有从库连接到主库的时候，主库都会创建一个线程然后发送binlog内容到从库。在从库里，当复制开始的时候，从库就会创建两个线程进行处理： 从库创建一个I/O线程，该线程连接到主库并请求主库发送binlog里面的更新记录到从库上。从库I/O线程读取主库的binlog输出线程发送的更新并拷贝这些更新到本地文件，其中包括relay log文件。 从库的SQL线程:从库创建一个SQL线程，这个线程读取从库I/O线程写到relay log的更新事件并执行。 问题&解决 主库宕机后，数据丢失 半同步复制 主库写压力大，因从库只有一个sql 线程来持久化，复制可能延迟 并行复制 复制出错处理 常见：1062（主键冲突），1032（记录不存在） 手动处理 OR 跳过复制错误：set global sql_slave_skip_counter=1 主从复制延迟 主节点如果执行一个很大的事务(更新千万行语句，总之执行很长时间的事务)，那么就会对主从延迟产生较大的影响 网络延迟，日志较大，slave数量过多。 主上多线程写入，从节点只有单线程恢复 处理办法： 大事务：将大事务分为小事务，分批更新数据。 减少Slave的数量，不要超过5个，减少单次事务的大小。 MySQL 5.7之后，可以使用多线程复制，使用MGR复制架构 半同步复制 原理:事务在主库写完binlog后需要从库返回一个已接受，才放回给客户端 5.5集成到mysql，以插件的形式存在，需要单独安装 确保事务提交后binlog至少传输到一个从库 不保证从库应用完成这个事务的binlog 性能有一定的降低 网络异常或从库宕机，卡主库，直到超时或从库恢复 并行复制 原理：从库多线程apply binlog 在社区5.6中新增 库级别并行应用binlog，同一个库数据更改还是串行的 5.7版本并行复制基于事务组 联级复制（部分数据复制） A->B->C B中添加参数log_slave_updates B将把A的binlog记录到自己的binlog日志中 "},"数据库/Mysql隐式类型转换.html":{"url":"数据库/Mysql隐式类型转换.html","title":"Mysql隐式类型转换","keywords":"","body":"当操作符与不同类型的操作数一起使用时，会发生类型转换以使操作数兼容。则会发生转换隐式 隐式类型转换规则 如果一个或两个参数都是NULL，比较的结果是NULL，除了NULL安全的相等比较运算符。对于NULL NULL，结果为true。不需要转换 如果比较操作中的两个参数都是字符串，则将它们作为字符串进行比较。 如果两个参数都是整数，则将它们作为整数进行比较。 如果不与数字进行比较，则将十六进制值视为二进制字符串 如果其中一个参数是十进制值，则比较取决于另一个参数。 如果另一个参数是十进制或整数值，则将参数与十进制值进行比较，如果另一个参数是浮点值，则将参数与浮点值进行比较 如果其中一个参数是TIMESTAMP或DATETIME列，另一个参数是常量，则在执行比较之前将常量转换为时间戳。 在所有其他情况下，参数都是作为浮点数（实数）比较的。 使用CAST函数显示转换 SELECT 38.8, CAST(38.8 AS CHAR); 字段为varchar类型,查询值为int类型会走索引吗? 不会(因为MySQL在对文本类型和数字类型进行比较的时候会进行隐式的类型转换,其他类型同理) int类型 使用unix_timestamp查询 走索引吗? 会 原因说明 以下是5.5官方手册的说明 If both arguments in a comparison operation are strings, they are compared as strings. 两个参数都是字符串，会按照字符串来比较，不做类型转换。 If both arguments are integers, they are compared as integers. 两个参数都是整数，按照整数来比较，不做类型转换。 Hexadecimal values are treated as binary strings if not compared to a number. 十六进制的值和非数字做比较时，会被当做二进制串。 If one of the arguments is a TIMESTAMP or DATETIME column and the other argument is a constant, the constant is converted to a timestamp before the comparison is performed. This is done to be more ODBC-friendly. Note that this is not done for the arguments to IN()! To be safe, always use complete datetime, date, or time strings when doing comparisons. For example, to achieve best results when using BETWEEN with date or time values, use CAST() to explicitly convert the values to the desired data type. 有一个参数是 TIMESTAMP 或 DATETIME，并且另外一个参数是常量，常量会被转换为 timestamp If one of the arguments is a decimal value, comparison depends on the other argument. The arguments are compared as decimal values if the other argument is a decimal or integer value, or as floating-point values if the other argument is a floating-point value. 有一个参数是 decimal 类型，如果另外一个参数是 decimal 或者整数，会将整数转换为 decimal 后进行比较，如果另外一个参数是浮点数，则会把 decimal 转换为浮点数进行比较 In all other cases, the arguments are compared as floating-point (real) numbers. 所有其他情况下，两个参数都会被转换为浮点数再进行比较 "},"数据库/Mysql的时间存储类型选择.html":{"url":"数据库/Mysql的时间存储类型选择.html","title":"Mysql的时间存储类型选择","keywords":"","body":"在实际开发项目中发现数据库中得日期项目，有的用datetime，有的用timestamp，有的甚至用int，这之间都有什么区别呢？什么场合适用什么样的设置呢？ int 占用4个字节 建立索引之后，查询速度快 条件范围搜索可以使用使用between 不能使用mysql提供的时间函数 可读性极差，无法直观的看到数据 datetime 占用8个字节 允许为空值，可以自定义值，系统不会自动修改其值。 与时区无关 不可以设定默认值，所以在不允许为空值的情况下，必须手动指定datetime字段的值才可以成功插入数据。 可以在指定datetime字段的值的时候使用now()变量来自动插入系统的当前时间。 以’YYYY-MM-DD HH:MM:SS’格式检索和显示DATETIME值。支持的范围为’1000-01-01 00:00:00’到’9999-12-31 23:59:59’ timestamp 占用4个字节 允许为空值，但是不可以自定义值，所以为空值时没有任何意义。 TIMESTAMP值不能早于1970或晚于2037。这说明一个日期，例如'1968-01-01'，虽然对于DATETIME或DATE值是有效的，但对于TIMESTAMP值却无效，如果分配给这样一个对象将被转换为0。 值以UTC格式保存 时区转化 ，存储时对当前的时区进行转换，检索时再转换回当前的时区。 默认值为CURRENT_TIMESTAMP()，其实也就是当前的系统时间。 数据库会自动修改其值，所以在插入记录时不需要指定timestamp字段的名称和timestamp字段的值，你只需要在设计表的时候添加一个timestamp字段即可，插入后该字段的值会自动变为当前系统时间。 以后任何时间修改表中的记录时，对应记录的timestamp值会自动被更新为当前的系统时间。 "},"数据库/Mysql基础数据类型详解.html":{"url":"数据库/Mysql基础数据类型详解.html","title":"Mysql基础数据类型详解","keywords":"","body":"数值类型 类型 大小 范围（有符号） 范围（无符号） 用途 TINYINT 1 字节 (-128，127) (0，255) 小整数值 SMALLINT 2 字节 (-32 768，32 767) (0，65 535) 大整数值 MEDIUMINT 3 字节 (-8 388 608，8 388 607) (0，16 777 215) 大整数值 INT或INTEGER 4 字节 (-2 147 483 648，2 147 483 647) (0，4 294 967 295) 大整数值 BIGINT 8 字节 (-9,223,372,036,854,775,808，9 223 372 036 854 775 807) (0，18 446 744 073 709 551 615) 极大整数值 FLOAT 4 字节 (-3.402 823 466 E+38，-1.175 494 351 E-38)，0，(1.175 494 351 E-38，3.402 823 466 351 E+38) 0，(1.175 494 351 E-38，3.402 823 466 E+38) 单精度,浮点数值 DOUBLE 8 字节 (-1.797 693 134 862 315 7 E+308，-2.225 073 858 507 201 4 E-308)，0，(2.225 073 858 507 201 4 E-308，1.797 693 134 862 315 7 E+308) 0，(2.225 073 858 507 201 4 E-308，1.797 693 134 862 315 7 E+308) 双精度,浮点数值 DECIMAL 对DECIMAL(M,D) ，如果M>D，为M+2否则为D+2 依赖于M和D的值 依赖于M和D的值 小数值 日期和时间类型 类型 大小(字节) 范围 格式 用途 DATE 3 1000-01-01/9999-12-31 YYYY-MM-DD 日期值 TIME 3 '-838:59:59'/'838:59:59' HH:MM:SS 时间值或持续时间 YEAR 1 1901/2155 YYYY 年份值 DATETIME 8 1000-01-01 00:00:00/9999-12-31 23:59:59 YYYY-MM-DD HH:MM:SS 混合日期和时间值 TIMESTAMP 4 1970-01-01 00:00:00/2038结束时间是第 2147483647 秒，北京时间 2038-1-19 11:14:07,格林尼治时间 2038年1月19日 凌晨 03:14:07 YYYYMMDD HHMMSS 混合日期和时间值，时间戳 字符串类型 类型 大小 用途 CHAR 0-255字节 定长字符串 VARCHAR 0-65535 字节 变长字符串 TINYBLOB 0-255字节 不超过 255 个字符的二进制字符串 TINYTEXT 0-255字节 短文本字符串 BLOB 0-65 535字节 二进制形式的长文本数据 TEXT 0-65 535字节 长文本数据 MEDIUMBLOB 0-16 777 215字节 二进制形式的中等长度文本数据 MEDIUMTEXT 0-16 777 215字节 中等长度文本数据 LONGBLOB 0-4 294 967 295字节 二进制形式的极大文本数据 LONGTEXT 0-4 294 967 295字节 极大文本数据 枚举、集合 ENUM （最多65535个成员） 64KB SET （最多64个成员） 64KB 一个汉字占多少长度与编码有关(MySQL 5.0 以上的版本) UTF－8：一个汉字＝3个字节 GBK：一个汉字＝2个字节 "},"数据库/Mysql死锁的产生&解决方案.html":{"url":"数据库/Mysql死锁的产生&解决方案.html","title":"Mysql死锁的产生&解决方案","keywords":"","body":"死锁产生条件 两个以上的并发事务 每个事务当前持有了锁，且未释放 每个事务都在申请新的锁 事务之间产生了锁资源的循环等待 第一种情况 一个用户A 访问表A(锁住了表A),然后又访问表B；另一个用户B 访问表B(锁住了表B)，然后企图访问表A；这时用户A由于用户B已经锁住表B，它必须等待用户B释放表B才能继续，同样用户B要等用户A释放表A才能继续，这就死锁就产生了。 [!TIP|label:解决方法] 这种死锁比较常见，是由于程序的BUG产生的，除了调整程序的逻辑没有其它的办法。仔细分析程序的逻辑，对于数据库的多表操作时，尽量按照相同的顺序进行处理，尽量避免同时锁定两个资源，如操作A和B两张表时，总是按先A后B的顺序处理， 必须同时锁定两个资源时，要保证在任何时刻都应该按照相同的顺序来锁定资源。 第二种情况 用户A查询一条纪录，然后修改该条纪录；这时用户B修改该条纪录，这时用户A的事务里锁的性质由查询的共享锁企图上升到独占锁，而用户B里的独占锁由于A 有共享锁存在所以必须等A释放掉共享锁，而A由于B的独占锁而无法上升的独占锁也就不可能释放共享锁，于是出现了死锁。这种死锁比较隐蔽，但在稍大点的项 目中经常发生。如在某项目中，页面上的按钮点击后，没有使按钮立刻失效，使得用户会多次快速点击同一按钮，这样同一段代码对数据库同一条记录进行多次操 作，很容易就出现这种死锁的情况。 [!TIP|label:解决方法] 对于按钮等控件，点击后使其立刻失效，不让用户重复点击，避免对同时对同一条记录操作。 使用乐观锁进行控制。乐观锁大多是基于数据版本（Version）记录机制实现。即为数据增加一个版本标识，在基于数据库表的版本解决方案中，一般是 通过为数据库表增加一个“version”字段来实现。读取出数据时，将此版本号一同读出，之后更新时，对此版本号加一。此时，将提交数据的版本数据与数 据库表对应记录的当前版本信息进行比对，如果提交的数据版本号大于数据库表当前版本号，则予以更新，否则认为是过期数据。乐观锁机制避免了长事务中的数据 库加锁开销（用户A和用户B操作过程中，都没有对数据库数据加锁），大大提升了大并发量下的系统整体性能表现。Hibernate 在其数据访问引擎中内置了乐观锁实现。需要注意的是，由于乐观锁机制是在我们的系统中实现，来自外部系统的用户更新操作不受我们系统的控制，因此可能会造 成脏数据被更新到数据库中。 使用悲观锁进行控制。悲观锁大多数情况下依靠数据库的锁机制实现，如Oracle的Select … for update语句，以保证操作最大程度的独占性。但随之而来的就是数据库性能的大量开销，特别是对长事务而言，这样的开销往往无法承受。如一个金融系统， 当某个操作员读取用户的数据，并在读出的用户数据的基础上进行修改时（如更改用户账户余额），如果采用悲观锁机制，也就意味着整个操作过程中（从操作员读 出数据、开始修改直至提交修改结果的全过程，甚至还包括操作员中途去煮咖啡的时间），数据库记录始终处于加锁状态，可以想见，如果面对成百上千个并发，这 样的情况将导致灾难性的后果。所以，采用悲观锁进行控制时一定要考虑清楚。 第三种情况 如果在事务中执行了一条不满足条件的update语句，则执行全表扫描，把行级锁上升为表级锁，多个这样的事务执行后，就很容易产生死锁和阻塞。类似的情 况还有当表中的数据量非常庞大而索引建的过少或不合适的时候，使得经常发生全表扫描，最终应用系统会越来越慢，最终发生阻塞或死锁。 [!TIP|label:解决方法] SQL语句中不要使用太复杂的关联多表的查询；使用“执行计划”对SQL语句进行分析，对于有全表扫描的SQL语句，建立相应的索引进行优化。 参考 解决死锁之路（终结篇） - 再见死锁 mysql数据库死锁的产生原因及解决办法 "},"Linux/Crontab表达式详解.html":{"url":"Linux/Crontab表达式详解.html","title":"Crontab表达式详解","keywords":"","body":"图解 特殊字符 特殊字符 代表意义 星号(*) 表示所有可能的值，可以理解为每。 逗号(,) 用逗号隔开的值表示一个列表范围，如1,2,3 每天每小时的第一、第二、第三分钟。 中杠(-) 用中杠隔开的值表示一个数值范围，如1-10 每天每小时的1到10分钟。 正斜线(/) 指定执行任务的间隔频率，如 0 10-18/2 *每天的十点到十八点间隔2小时执行。 实例 # 每分钟执行一次 * * * * * # 每小时的第3和第15分钟执行 3,15 * * * * #在上午的8点到11点的第3和第15分钟执行 3,15 8-11 * * * #在每隔2天的上午8点和11点的第3和第15分钟执行 3,15 8-11 */2 * * #每个星期一的上午8点到11点的第3和第15分钟执行 3,15 8-11 * * 1 #每晚的21：30执行 30 21 * * * #每月1、10、22日的4:30执行 30 4 1,10,22 * * #每周六、日1:10执行 10 1 * * 6,7 #每天18:00到23:00之间每隔30分钟执行 0/30 18-23 * * * #星期六的23:00执行 0 23 * * 6 #每小时执行一次 * */1 * * * #晚上11点到早上7点之间，每小时执行一次 * 23-7/1 * * * #每月的4号与每周一到周三的11点 0 11 4 * 1-3 #一月一号的4点 0 4 1 1 * "},"Linux/Linux文件权限说明.html":{"url":"Linux/Linux文件权限说明.html","title":"Linux文件权限说明","keywords":"","body":"位置说明 第一个字段 - 代表文件 d 代表目录 l 代表链接 c 代表字符型设备 b 代表块设备 n 代表网络设备 权限值说明 权限 权限数值 二进制 具体作用 r 4 00000100 read，读取。当前用户可以读取文件内容，当前用户可以浏览目录。 w 2 00000010 write，写入。当前用户可以新增或修改文件内容，当前用户可以删除、移动目录或目录内文件。 x 1 00000001 execute，执行。当前用户可以执行文件，当前用户可以进入目录。 "},"其他/TCP的三次握手&四次挥手.html":{"url":"其他/TCP的三次握手&四次挥手.html","title":"TCP的三次握手&四次挥手","keywords":"","body":" 第一次握手(SYN=1, seq=x): 客户端发送一个 TCP 的 SYN 标志位置1的包，指明客户端打算连接的服务器的端口，以及初始序号 X,保存在包头的序列号(Sequence Number)字段里。 发送完毕后，客户端进入 SYN_SEND 状态。 第二次握手(SYN=1, ACK=1, seq=y, ACKnum=x+1): 服务器发回确认包(ACK)应答。即 SYN 标志位和 ACK 标志位均为1。服务器端选择自己 ISN 序列号，放到 Seq 域里，同时将确认序号(Acknowledgement Number)设置为客户的 ISN 加1，即X+1。 - 发送完毕后，服务器端进入 SYN_RCVD 状态。 第三次握手(ACK=1，ACKnum=y+1) 客户端再次发送确认包(ACK)，SYN 标志位为0，ACK 标志位为1，并且把服务器发来 ACK 的序号字段+1，放在确定字段中发送给对方，并且在数据段放写ISN的+1 发送完毕后，客户端进入 ESTABLISHED 状态，当服务器端接收到这个包时，也进入 ESTABLISHED 状态，TCP 握手结束。 第一次挥手(FIN=1，seq=x) 假设客户端想要关闭连接，客户端发送一个 FIN 标志位置为1的包，表示自己已经没有数据可以发送了，但是仍然可以接受数据。 发送完毕后，客户端进入 FIN_WAIT_1 状态。 第二次挥手(ACK=1，ACKnum=x+1) 服务器端确认客户端的 FIN 包，发送一个确认包，表明自己接受到了客户端关闭连接的请求，但还没有准备好关闭连接。 发送完毕后，服务器端进入 CLOSE_WAIT 状态，客户端接收到这个确认包之后，进入 FIN_WAIT_2 状态，等待服务器端关闭连接。 第三次挥手(FIN=1，seq=y) 服务器端准备好关闭连接时，向客户端发送结束连接请求，FIN 置为1。 发送完毕后，服务器端进入 LAST_ACK 状态，等待来自客户端的最后一个ACK。 第四次挥手(ACK=1，ACKnum=y+1) 客户端接收到来自服务器端的关闭请求，发送一个确认包，并进入 TIME_WAIT状态，等待可能出现的要求重传的 ACK 包。 服务器端接收到这个确认包之后，关闭连接，进入 CLOSED 状态。 客户端等待了某个固定时间（两个最大段生命周期，2MSL，2 Maximum Segment Lifetime）之后，没有收到服务器端的 ACK ，认为服务器端已经正常关闭连接，于是自己也关闭连接，进入 CLOSED 状态。 "},"其他/POST&GET区别.html":{"url":"其他/POST&GET区别.html","title":"POST&GET区别","keywords":"","body":"GET - 从指定的资源请求数据。 GET 请求可被缓存 GET 请求保留在浏览器历史记录中 GET 请求可被收藏为书签 GET 请求不应在处理敏感数据时使用 GET 请求有长度限制 GET 请求只应当用于取回数据 GET 只允许 ASCII 字符。 POST - 向指定的资源提交要被处理的数据 POST 请求不会被缓存 POST 请求不会保留在浏览器历史记录中 POST 不能被收藏为书签 POST 请求对数据长度没有要求 POST 数据类型没有限制 对比 区别 GET POST 后退按钮&刷新 无害 数据会被重新提交(浏览器应该告知用户数据会被重新提交) 书签 可收藏为书签 不可收藏为书签 缓存 能被缓存 不能缓存 编码类型 application/x-www-form-urlencoded application/x-www-form-urlencoded 或 multipart/form-data。为二进制数据使用多重编码。 历史 参数保留在浏览器历史中。 参数不会保存在浏览器历史中。 对数据长度的限制 是的。当发送数据时，GET 方法向 URL 添加数据；URL 的长度是受限制的（URL 的最大长度是 2048 个字符）。 无限制。 对数据类型的限制 只允许 ASCII 字符。 没有限制。也允许二进制数据。 安全性 所发送的数据是 URL 的一部分。 数据不会显示在 URL 中 其他 HTTP 请求方法 方法 描述 HEAD 与 GET 相同，但只返回 HTTP 报头，不返回文档主体。 PUT 上传指定的 URI 表示。 DELETE 删除指定资源。 OPTIONS 返回服务器支持的 HTTP 方法。 CONNECT 把请求连接转换到透明的 TCP/IP 通道。 "},"其他/Http状态码.html":{"url":"其他/Http状态码.html","title":"Http状态码","keywords":"","body":"1XX ：信息状态码 100 Continue 继续，一般在发送 post 请求时，已发送了 http header 之后服务端 将返回此信息，表示确认，之后发送具体参数信息 2XX ：成功状态码 200 OK 正常返回信息 201 Created 请求成功并且服务器创建了新的资源 202 Accepted 服务器已接受请求，但尚未处理 3XX ：重定向 301 Moved Permanently 请求的网页已永久移动到新位置。 302 Found 临时性重定向。 303 See Other 临时性重定向，且总是使用 GET 请求新的 URI 。 304 Not Modified 自从上次请求后，请求的网页未修改过。 4XX ：客户端错误 400 Bad Request 服务器无法理解请求的格式，客户端不应当尝试再次使用相同的 内容发起请求。 401 Unauthorized 请求未授权。 403 Forbidden 禁止访问。 404 Not Found 找不到如何与 URI 相匹配的资源。 5XX: 服务器错误 500 Internal Server Error 最常见的服务器端错误。 503 Service Unavailable 服务器端暂时无法处理请求（可能是过载或维护）。 参考列表 状态码 含义 100 客户端应当继续发送请求。这个临时响应是用来通知客户端它的部分请求已经被服务器接收，且仍未被拒绝。客户端应当继续发送请求的剩余部分，或者如果请求已经完成，忽略这个响应。服务器必须在请求完成后向客户端发送一个最终响应。 101 服务器已经理解了客户端的请求，并将通过Upgrade 消息头通知客户端采用不同的协议来完成这个请求。在发送完这个响应最后的空行后，服务器将会切换到在Upgrade 消息头中定义的那些协议。 　　只有在切换新的协议更有好处的时候才应该采取类似措施。例如，切换到新的HTTP 版本比旧版本更有优势，或者切换到一个实时且同步的协议以传送利用此类特性的资源。 102 由WebDAV（RFC 2518）扩展的状态码，代表处理将被继续执行。 200 请求已成功，请求所希望的响应头或数据体将随此响应返回。 201 请求已经被实现，而且有一个新的资源已经依据请求的需要而建立，且其 URI 已经随Location 头信息返回。假如需要的资源无法及时建立的话，应当返回 '202 Accepted'。 202 服务器已接受请求，但尚未处理。正如它可能被拒绝一样，最终该请求可能会也可能不会被执行。在异步操作的场合下，没有比发送这个状态码更方便的做法了。 　　返回202状态码的响应的目的是允许服务器接受其他过程的请求（例如某个每天只执行一次的基于批处理的操作），而不必让客户端一直保持与服务器的连接直到批处理操作全部完成。在接受请求处理并返回202状态码的响应应当在返回的实体中包含一些指示处理当前状态的信息，以及指向处理状态监视器或状态预测的指针，以便用户能够估计操作是否已经完成。 203 服务器已成功处理了请求，但返回的实体头部元信息不是在原始服务器上有效的确定集合，而是来自本地或者第三方的拷贝。当前的信息可能是原始版本的子集或者超集。例如，包含资源的元数据可能导致原始服务器知道元信息的超级。使用此状态码不是必须的，而且只有在响应不使用此状态码便会返回200 OK的情况下才是合适的。 204 服务器成功处理了请求，但不需要返回任何实体内容，并且希望返回更新了的元信息。响应可能通过实体头部的形式，返回新的或更新后的元信息。如果存在这些头部信息，则应当与所请求的变量相呼应。 　　如果客户端是浏览器的话，那么用户浏览器应保留发送了该请求的页面，而不产生任何文档视图上的变化，即使按照规范新的或更新后的元信息应当被应用到用户浏览器活动视图中的文档。 　　由于204响应被禁止包含任何消息体，因此它始终以消息头后的第一个空行结尾。 205 服务器成功处理了请求，且没有返回任何内容。但是与204响应不同，返回此状态码的响应要求请求者重置文档视图。该响应主要是被用于接受用户输入后，立即重置表单，以便用户能够轻松地开始另一次输入。 　　与204响应一样，该响应也被禁止包含任何消息体，且以消息头后的第一个空行结束。 206 服务器已经成功处理了部分 GET 请求。类似于 FlashGet 或者迅雷这类的 HTTP 下载工具都是使用此类响应实现断点续传或者将一个大文档分解为多个下载段同时下载。 　　该请求必须包含 Range 头信息来指示客户端希望得到的内容范围，并且可能包含 If-Range 来作为请求条件。 　　响应必须包含如下的头部域： 　　Content-Range 用以指示本次响应中返回的内容的范围；如果是 Content-Type 为 multipart/byteranges 的多段下载，则每一 multipart 段中都应包含 Content-Range 域用以指示本段的内容范围。假如响应中包含 Content-Length，那么它的数值必须匹配它返回的内容范围的真实字节数。 　　Date 　　ETag 和/或 Content-Location，假如同样的请求本应该返回200响应。 　　Expires, Cache-Control，和/或 Vary，假如其值可能与之前相同变量的其他响应对应的值不同的话。 　　假如本响应请求使用了 If-Range 强缓存验证，那么本次响应不应该包含其他实体头；假如本响应的请求使用了 If-Range 弱缓存验证，那么本次响应禁止包含其他实体头；这避免了缓存的实体内容和更新了的实体头信息之间的不一致。否则，本响应就应当包含所有本应该返回200响应中应当返回的所有实体头部域。 　　假如 ETag 或 Last-Modified 头部不能精确匹配的话，则客户端缓存应禁止将206响应返回的内容与之前任何缓存过的内容组合在一起。 　　任何不支持 Range 以及 Content-Range 头的缓存都禁止缓存206响应返回的内容。 207 由WebDAV(RFC 2518)扩展的状态码，代表之后的消息体将是一个XML消息，并且可能依照之前子请求数量的不同，包含一系列独立的响应代码。 300 被请求的资源有一系列可供选择的回馈信息，每个都有自己特定的地址和浏览器驱动的商议信息。用户或浏览器能够自行选择一个首选的地址进行重定向。 　　除非这是一个 HEAD 请求，否则该响应应当包括一个资源特性及地址的列表的实体，以便用户或浏览器从中选择最合适的重定向地址。这个实体的格式由 Content-Type 定义的格式所决定。浏览器可能根据响应的格式以及浏览器自身能力，自动作出最合适的选择。当然，RFC 2616规范并没有规定这样的自动选择该如何进行。 　　如果服务器本身已经有了首选的回馈选择，那么在 Location 中应当指明这个回馈的 URI；浏览器可能会将这个 Location 值作为自动重定向的地址。此外，除非额外指定，否则这个响应也是可缓存的。 301 被请求的资源已永久移动到新位置，并且将来任何对此资源的引用都应该使用本响应返回的若干个 URI 之一。如果可能，拥有链接编辑功能的客户端应当自动把请求的地址修改为从服务器反馈回来的地址。除非额外指定，否则这个响应也是可缓存的。 　　新的永久性的 URI 应当在响应的 Location 域中返回。除非这是一个 HEAD 请求，否则响应的实体中应当包含指向新的 URI 的超链接及简短说明。 　　如果这不是一个 GET 或者 HEAD 请求，因此浏览器禁止自动进行重定向，除非得到用户的确认，因为请求的条件可能因此发生变化。 　　注意：对于某些使用 HTTP/1.0 协议的浏览器，当它们发送的 POST 请求得到了一个301响应的话，接下来的重定向请求将会变成 GET 方式。 302 请求的资源现在临时从不同的 URI 响应请求。由于这样的重定向是临时的，客户端应当继续向原有地址发送以后的请求。只有在Cache-Control或Expires中进行了指定的情况下，这个响应才是可缓存的。 　　新的临时性的 URI 应当在响应的 Location 域中返回。除非这是一个 HEAD 请求，否则响应的实体中应当包含指向新的 URI 的超链接及简短说明。 　　如果这不是一个 GET 或者 HEAD 请求，那么浏览器禁止自动进行重定向，除非得到用户的确认，因为请求的条件可能因此发生变化。 　　注意：虽然RFC 1945和RFC 2068规范不允许客户端在重定向时改变请求的方法，但是很多现存的浏览器将302响应视作为303响应，并且使用 GET 方式访问在 Location 中规定的 URI，而无视原先请求的方法。状态码303和307被添加了进来，用以明确服务器期待客户端进行何种反应。 303 对应当前请求的响应可以在另一个 URI 上被找到，而且客户端应当采用 GET 的方式访问那个资源。这个方法的存在主要是为了允许由脚本激活的POST请求输出重定向到一个新的资源。这个新的 URI 不是原始资源的替代引用。同时，303响应禁止被缓存。当然，第二个请求（重定向）可能被缓存。 　　新的 URI 应当在响应的 Location 域中返回。除非这是一个 HEAD 请求，否则响应的实体中应当包含指向新的 URI 的超链接及简短说明。 　　注意：许多 HTTP/1.1 版以前的 浏览器不能正确理解303状态。如果需要考虑与这些浏览器之间的互动，302状态码应该可以胜任，因为大多数的浏览器处理302响应时的方式恰恰就是上述规范要求客户端处理303响应时应当做的。 304 如果客户端发送了一个带条件的 GET 请求且该请求已被允许，而文档的内容（自上次访问以来或者根据请求的条件）并没有改变，则服务器应当返回这个状态码。304响应禁止包含消息体，因此始终以消息头后的第一个空行结尾。 　　该响应必须包含以下的头信息： 　　Date，除非这个服务器没有时钟。假如没有时钟的服务器也遵守这些规则，那么代理服务器以及客户端可以自行将 Date 字段添加到接收到的响应头中去（正如RFC 2068中规定的一样），缓存机制将会正常工作。 　　ETag 和/或 Content-Location，假如同样的请求本应返回200响应。 　　Expires, Cache-Control，和/或Vary，假如其值可能与之前相同变量的其他响应对应的值不同的话。 　　假如本响应请求使用了强缓存验证，那么本次响应不应该包含其他实体头；否则（例如，某个带条件的 GET 请求使用了弱缓存验证），本次响应禁止包含其他实体头；这避免了缓存了的实体内容和更新了的实体头信息之间的不一致。 　　假如某个304响应指明了当前某个实体没有缓存，那么缓存系统必须忽视这个响应，并且重复发送不包含限制条件的请求。 　　假如接收到一个要求更新某个缓存条目的304响应，那么缓存系统必须更新整个条目以反映所有在响应中被更新的字段的值。 305 被请求的资源必须通过指定的代理才能被访问。Location 域中将给出指定的代理所在的 URI 信息，接收者需要重复发送一个单独的请求，通过这个代理才能访问相应资源。只有原始服务器才能建立305响应。 　　注意：RFC 2068中没有明确305响应是为了重定向一个单独的请求，而且只能被原始服务器建立。忽视这些限制可能导致严重的安全后果。 306 在最新版的规范中，306状态码已经不再被使用。 307 请求的资源现在临时从不同的URI 响应请求。由于这样的重定向是临时的，客户端应当继续向原有地址发送以后的请求。只有在Cache-Control或Expires中进行了指定的情况下，这个响应才是可缓存的。 　　新的临时性的URI 应当在响应的 Location 域中返回。除非这是一个HEAD 请求，否则响应的实体中应当包含指向新的URI 的超链接及简短说明。因为部分浏览器不能识别307响应，因此需要添加上述必要信息以便用户能够理解并向新的 URI 发出访问请求。 　　如果这不是一个GET 或者 HEAD 请求，那么浏览器禁止自动进行重定向，除非得到用户的确认，因为请求的条件可能因此发生变化。 400 1、语义有误，当前请求无法被服务器理解。除非进行修改，否则客户端不应该重复提交这个请求。 　　2、请求参数有误。 401 当前请求需要用户验证。该响应必须包含一个适用于被请求资源的 WWW-Authenticate 信息头用以询问用户信息。客户端可以重复提交一个包含恰当的 Authorization 头信息的请求。如果当前请求已经包含了 Authorization 证书，那么401响应代表着服务器验证已经拒绝了那些证书。如果401响应包含了与前一个响应相同的身份验证询问，且浏览器已经至少尝试了一次验证，那么浏览器应当向用户展示响应中包含的实体信息，因为这个实体信息中可能包含了相关诊断信息。参见RFC 2617。 402 该状态码是为了将来可能的需求而预留的。 403 服务器已经理解请求，但是拒绝执行它。与401响应不同的是，身份验证并不能提供任何帮助，而且这个请求也不应该被重复提交。如果这不是一个 HEAD 请求，而且服务器希望能够讲清楚为何请求不能被执行，那么就应该在实体内描述拒绝的原因。当然服务器也可以返回一个404响应，假如它不希望让客户端获得任何信息。 404 请求失败，请求所希望得到的资源未被在服务器上发现。没有信息能够告诉用户这个状况到底是暂时的还是永久的。假如服务器知道情况的话，应当使用410状态码来告知旧资源因为某些内部的配置机制问题，已经永久的不可用，而且没有任何可以跳转的地址。404这个状态码被广泛应用于当服务器不想揭示到底为何请求被拒绝或者没有其他适合的响应可用的情况下。 405 请求行中指定的请求方法不能被用于请求相应的资源。该响应必须返回一个Allow 头信息用以表示出当前资源能够接受的请求方法的列表。 　　鉴于 PUT，DELETE 方法会对服务器上的资源进行写操作，因而绝大部分的网页服务器都不支持或者在默认配置下不允许上述请求方法，对于此类请求均会返回405错误。 406 请求的资源的内容特性无法满足请求头中的条件，因而无法生成响应实体。 　　除非这是一个 HEAD 请求，否则该响应就应当返回一个包含可以让用户或者浏览器从中选择最合适的实体特性以及地址列表的实体。实体的格式由 Content-Type 头中定义的媒体类型决定。浏览器可以根据格式及自身能力自行作出最佳选择。但是，规范中并没有定义任何作出此类自动选择的标准。 407 　与401响应类似，只不过客户端必须在代理服务器上进行身份验证。代理服务器必须返回一个 Proxy-Authenticate 用以进行身份询问。客户端可以返回一个 Proxy-Authorization 信息头用以验证。参见RFC 2617。 408 请求超时。客户端没有在服务器预备等待的时间内完成一个请求的发送。客户端可以随时再次提交这一请求而无需进行任何更改。 409 由于和被请求的资源的当前状态之间存在冲突，请求无法完成。这个代码只允许用在这样的情况下才能被使用：用户被认为能够解决冲突，并且会重新提交新的请求。该响应应当包含足够的信息以便用户发现冲突的源头。 　　冲突通常发生于对 PUT 请求的处理中。例如，在采用版本检查的环境下，某次 PUT 提交的对特定资源的修改请求所附带的版本信息与之前的某个（第三方）请求向冲突，那么此时服务器就应该返回一个409错误，告知用户请求无法完成。此时，响应实体中很可能会包含两个冲突版本之间的差异比较，以便用户重新提交归并以后的新版本。 410 被请求的资源在服务器上已经不再可用，而且没有任何已知的转发地址。这样的状况应当被认为是永久性的。如果可能，拥有链接编辑功能的客户端应当在获得用户许可后删除所有指向这个地址的引用。如果服务器不知道或者无法确定这个状况是否是永久的，那么就应该使用404状态码。除非额外说明，否则这个响应是可缓存的。 　　410响应的目的主要是帮助网站管理员维护网站，通知用户该资源已经不再可用，并且服务器拥有者希望所有指向这个资源的远端连接也被删除。这类事件在限时、增值服务中很普遍。同样，410响应也被用于通知客户端在当前服务器站点上，原本属于某个个人的资源已经不再可用。当然，是否需要把所有永久不可用的资源标记为'410 Gone'，以及是否需要保持此标记多长时间，完全取决于服务器拥有者。 411 服务器拒绝在没有定义 Content-Length 头的情况下接受请求。在添加了表明请求消息体长度的有效 Content-Length 头之后，客户端可以再次提交该请求。 412 服务器在验证在请求的头字段中给出先决条件时，没能满足其中的一个或多个。这个状态码允许客户端在获取资源时在请求的元信息（请求头字段数据）中设置先决条件，以此避免该请求方法被应用到其希望的内容以外的资源上。 413 服务器拒绝处理当前请求，因为该请求提交的实体数据大小超过了服务器愿意或者能够处理的范围。此种情况下，服务器可以关闭连接以免客户端继续发送此请求。 　　如果这个状况是临时的，服务器应当返回一个 Retry-After 的响应头，以告知客户端可以在多少时间以后重新尝试。 414 请求的URI 长度超过了服务器能够解释的长度，因此服务器拒绝对该请求提供服务。这比较少见，通常的情况包括： 　　本应使用POST方法的表单提交变成了GET方法，导致查询字符串（Query String）过长。 　　重定向URI “黑洞”，例如每次重定向把旧的 URI 作为新的 URI 的一部分，导致在若干次重定向后 URI 超长。 　　客户端正在尝试利用某些服务器中存在的安全漏洞攻击服务器。这类服务器使用固定长度的缓冲读取或操作请求的 URI，当 GET 后的参数超过某个数值后，可能会产生缓冲区溢出，导致任意代码被执行[1]。没有此类漏洞的服务器，应当返回414状态码。 415 对于当前请求的方法和所请求的资源，请求中提交的实体并不是服务器中所支持的格式，因此请求被拒绝。 416 如果请求中包含了 Range 请求头，并且 Range 中指定的任何数据范围都与当前资源的可用范围不重合，同时请求中又没有定义 If-Range 请求头，那么服务器就应当返回416状态码。 　　假如 Range 使用的是字节范围，那么这种情况就是指请求指定的所有数据范围的首字节位置都超过了当前资源的长度。服务器也应当在返回416状态码的同时，包含一个 Content-Range 实体头，用以指明当前资源的长度。这个响应也被禁止使用 multipart/byteranges 作为其 Content-Type。 417 在请求头 Expect 中指定的预期内容无法被服务器满足，或者这个服务器是一个代理服务器，它有明显的证据证明在当前路由的下一个节点上，Expect 的内容无法被满足。 421 从当前客户端所在的IP地址到服务器的连接数超过了服务器许可的最大范围。通常，这里的IP地址指的是从服务器上看到的客户端地址（比如用户的网关或者代理服务器地址）。在这种情况下，连接数的计算可能涉及到不止一个终端用户。 422 从当前客户端所在的IP地址到服务器的连接数超过了服务器许可的最大范围。通常，这里的IP地址指的是从服务器上看到的客户端地址（比如用户的网关或者代理服务器地址）。在这种情况下，连接数的计算可能涉及到不止一个终端用户。 422 请求格式正确，但是由于含有语义错误，无法响应。（RFC 4918 WebDAV）423 Locked 　　当前资源被锁定。（RFC 4918 WebDAV） 424 由于之前的某个请求发生的错误，导致当前请求失败，例如 PROPPATCH。（RFC 4918 WebDAV） 425 在WebDav Advanced Collections 草案中定义，但是未出现在《WebDAV 顺序集协议》（RFC 3658）中。 426 客户端应当切换到TLS/1.0。（RFC 2817） 449 由微软扩展，代表请求应当在执行完适当的操作后进行重试。 500 服务器遇到了一个未曾预料的状况，导致了它无法完成对请求的处理。一般来说，这个问题都会在服务器的程序码出错时出现。 501 服务器不支持当前请求所需要的某个功能。当服务器无法识别请求的方法，并且无法支持其对任何资源的请求。 502 作为网关或者代理工作的服务器尝试执行请求时，从上游服务器接收到无效的响应。 503 由于临时的服务器维护或者过载，服务器当前无法处理请求。这个状况是临时的，并且将在一段时间以后恢复。如果能够预计延迟时间，那么响应中可以包含一个 Retry-After 头用以标明这个延迟时间。如果没有给出这个 Retry-After 信息，那么客户端应当以处理500响应的方式处理它。 　　注意：503状态码的存在并不意味着服务器在过载的时候必须使用它。某些服务器只不过是希望拒绝客户端的连接。 504 作为网关或者代理工作的服务器尝试执行请求时，未能及时从上游服务器（URI标识出的服务器，例如HTTP、FTP、LDAP）或者辅助服务器（例如DNS）收到响应。 　　注意：某些代理服务器在DNS查询超时时会返回400或者500错误 505 服务器不支持，或者拒绝支持在请求中使用的 HTTP 版本。这暗示着服务器不能或不愿使用与客户端相同的版本。响应中应当包含一个描述了为何版本不被支持以及服务器支持哪些协议的实体。 506 由《透明内容协商协议》（RFC 2295）扩展，代表服务器存在内部配置错误：被请求的协商变元资源被配置为在透明内容协商中使用自己，因此在一个协商处理中不是一个合适的重点。 507 服务器无法存储完成请求所必须的内容。这个状况被认为是临时的。WebDAV (RFC 4918) 509 服务器达到带宽限制。这不是一个官方的状态码，但是仍被广泛使用。 510 获取资源所需要的策略并没有没满足。（RFC 2774） "},"其他/OSI七层&五层协议.html":{"url":"其他/OSI七层&五层协议.html","title":"OSI七层&五层协议","keywords":"","body":" OSI分层(7层) 物理层 数据链路层 网络层 传输层 会话层 表示层 应用层。 TCP/IP分层(4层) 网络接口层 网际层 运输层 应用层 五层协议(5层) 物理层 数据链路层 网络层 运输层 应用层 "},"其他/栈和队列的区别.html":{"url":"其他/栈和队列的区别.html","title":"栈和队列的区别","keywords":"","body":"栈（Stack）和队列（Queue）是两种操作受限的线性表。 相同点 都是线性结构。 都可以通过顺序结构和链式结构实现。 插入与删除的时间复杂度都是O(1),在空间复杂度上两者也一样。 不同点 栈的插入和删除操作都是在一端进行的，而队列的操作却是在两端进行的。 队列先进先出，栈先进后出。 栈只允许在表尾一端进行插入和删除，而队列只允许在表尾一端进行插入，在表头一端进行删除 应用场景不同；常见栈的应用场景包括括号问题的求解，表达式的转换和求值，函数调用和递归实现，深度优先搜索遍历等；常见的队列的应用场景包括计算机系统中各种资源的管理，消息缓冲器的管理和广度优先搜索遍历等。 顺序栈能够实现多栈空间共享，而顺序队列不能。 "},"PHP/Nginx&PHP通信.html":{"url":"PHP/Nginx&PHP通信.html","title":"Nginx&PHP通信","keywords":"","body":"PHP-FPM PHP-FPM 即 PHP-FastCGI Process Manager， 它是 FastCGI 的实现，并提供了进程管理的功能。进程包含 master 进程和 worker 进程两种；master 进程只有一个，负责监听端口，接收来自服务器的请求，而 worker 进程则一般有多个（具体数量根据实际需要进行配置），每个进程内部都会嵌入一个 PHP 解释器，是代码真正执行的地方。 nginx与php-fpm通信方式 在 Linux 上，nginx 与 php-fpm 的通信有 tcp socket 和 unix socket 两种方式。 tcp socket tcp socket 的优点是可以跨服务器，当 nginx 和 php-fpm 不在同一台机器上时，只能使用这种方式 unix socket [!NOTE|label:注意] 在使用 unix socket 方式连接时，由于 socket 文件本质上是一个文件，存在权限控制的问题，所以需要注意 nginx 进程的权限与 php-fpm 的权限问题，不然会提示无权限访问。（在各自的配置文件里设置用户） Unix socket 又叫 IPC(inter-process communication 进程间通信) socket，用于实现同一主机上的进程间通信，这种方式需要在 nginx配置文件中填写 php-fpm 的 socket 文件位置。 区别 由于 Unix socket 不需要经过网络协议栈，不需要打包拆包、计算校验和、维护序号和应答等，只是将应用层数据从一个进程拷贝到另一个进程。所以其效率比 tcp socket 的方式要高，可减少不必要的 tcp 开销。不过，unix socket 高并发时不稳定，连接数爆发时，会产生大量的长时缓存，在没有面向连接协议的支撑下，大数据包可能会直接出错不返回异常。而 tcp 这样的面向连接的协议，可以更好的保证通信的正确性和完整性。 在应用中的选择 如果是在同一台服务器上运行的 nginx 和 php-fpm，且并发量不高（不超过1000），选择unix socket，以提高 nginx 和 php-fpm 的通信效率。 如果是面临高并发业务，则考虑选择使用更可靠的 tcp socket，以负载均衡、内核优化等运维手段维持效率。 若并发较高但仍想用 unix socket 时，可通过以下方式提高 unix socket 的稳定性。 1）将sock文件放在 /dev/shm 目录下，此目录下将 sock 文件放在内存里面，内存的读写更快。 2）提高 backlog backlog 默认位 128，1024 这个值最好换算成自己正常的 QPS，配置如下。 Nginx&FPM处理流程 开始 | | www.example.com/index.php | | nginx 80 端口 | | nginx 加载 fastcgi 模块 | | 反向代理到 fpm 监听的 9000 端口 | | fpm 处理请求并返回至 nginx | | nginx 接收并返回客户端 | | 结束 "},"PHP/守护进程的原理与实现.html":{"url":"PHP/守护进程的原理与实现.html","title":"守护进程的原理与实现","keywords":"","body":"在linux或者unix操作系统中在系统的引导的时候会开启很多服务，这些服务就叫做守护进程。为了增加灵活性，root可以选择系统开启的模式，这些模式叫做运行级别，每一种运行级别以一定的方式配置系统。 守护进程是脱离于终端并且在后台运行的进程。守护进程脱离于终端是为了避免进程在执行过程中的信息在任何终端上显示并且进程也不会被任何终端所产生的终端信息所打断。 守护进程及其特性 守 护进程最重要的特性是后台运行。在这一点上DOS下的常驻内存程序TSR与之相似。其次，守护进程必须与其运行前的环境隔离开来。这些环境包括未关闭的文 件描述符，控制终端，会话和进程组，工作目录以及文件创建掩模等。这些环境通常是守护进程从执行它的父进程（特别是shell）中继承下来的。最后，守护 进程的启动方式有其特殊之处。它可以在Linux系统启动时从启动脚本/etc/rc.d中启动，可以由作业规划进程crond启动，还可以由用户终端 （通常是shell）执行。 守护进程流程处理 创建子进程，父进程退出 在子进程中创建新会话 改变当前目录为根目 重设文件权限掩码 关闭文件描述符 守护进程退出处理 PHP参考 0 ) { exit(); } // 在子进程中创建新会话 if( !posix_setsid() ){ exit('setsid error.'); } $pid = pcntl_fork(); if( $pid 0 ) { exit; } // 改变工作目录 chdir( '/tmp' ); // 关闭文件描述符 fclose($fatherFile); C参考 #include #include #include #include #include #include #include #include #define MAXFILE 65535 volatile sig_atomic_t _running = 1; void sigterm_handler(int arg) { _running = 0; } int main() { pid_t pc, pid; int i, fd, len; char *buf = \"this is a Dameon\\n\"; len = strlen(buf); //第一步 pc = fork(); if(pc 0) { exit(0); } //第二步 setsid(); pid = fork();//与终端完全脱离[1] if (pid 0) { exit(0); } //第三步 chdir(\"/\"); //第四步 umask(0); //第五步 for(i = 0;i Linux守护进程的原理与实现 Linux守护进程的编程方法 PHP实现守护进程 "},"PHP/Cookie&Session区别.html":{"url":"PHP/Cookie&Session区别.html","title":"Cookie&Session区别","keywords":"","body":"1、存储位置不同 cookie的数据信息存放在客户端浏览器上。 session的数据信息存放在服务器上。 2、存储容量不同 单个cookie保存的数据 对于session来说并没有上限，但出于对服务器端的性能考虑，session内不要存放过多的东西，并且设置session删除机制。 3、存储方式不同 cookie中只能保管ASCII字符串，并需要通过编码方式存储为Unicode字符或者二进制数据。 session中能够存储任何类型的数据，包括且不限于string，integer，list，map等。 4、隐私策略不同 cookie对客户端是可见的，别有用心的人可以分析存放在本地的cookie并进行cookie欺骗，所以它是不安全的。 session存储在服务器上，对客户端是透明对，不存在敏感信息泄漏的风险。 5、有效期上不同 开发可以通过设置cookie的属性，达到使cookie长期有效的效果。 session依赖于名为JSESSIONID的cookie，而cookie JSESSIONID的过期时间默认为-1，只需关闭窗口该session就会失效，因而session不能达到长期有效的效果。 6、服务器压力不同 cookie保管在客户端，不占用服务器资源。对于并发用户十分多的网站，cookie是很好的选择。 session是保管在服务器端的，每个用户都会产生一个session。假如并发访问的用户十分多，会产生十分多的session，耗费大量的内存。 7、浏览器支持不同 假如客户端浏览器不支持cookie： cookie是需要客户端浏览器支持的，假如客户端禁用了cookie，或者不支持cookie，则会话跟踪会失效。关于WAP上的应用，常规的cookie就派不上用场了。 运用session需要使用URL地址重写的方式。一切用到session程序的URL都要进行URL地址重写，否则session会话跟踪还会失效。 假如客户端支持cookie： cookie既能够设为本浏览器窗口以及子窗口内有效，也能够设为一切窗口内有效。 session只能在本窗口以及子窗口内有效。 8、跨域支持上不同 cookie支持跨域名访问。 session不支持跨域名访问。 Cookie的工作机制 服务器向客户端响应请求的时候，会在响应头中设置set-cookie的值，其值的格式通常是name = value的格式 浏览器将 cookie 保存下来 每次请求浏览器都会自动将 cookie 发向服务器 cookie最初是在客户端用于存储会话信息的。 Session的工作机制 当客户端第一次请求session对象时，服务器会创建一个session，并通过特殊算法算出一个session的ID，用来标识该session对象，然后将这个session序列放置到set-cookie中发送给浏览器 浏览器下次发请求的时候，这个sessionID会被放置在请求头中，和cookie一起发送回来 服务器再通过内存中保存的sessionID跟cookie中保存的sessionID进行比较，并根据ID在内存中找到之前创建的session对象，提供给请求使用，也就是服务器会通过session保存一个状态记录，浏览器会通过cookie保存状态记录，服务器通过两者的对比实现跟踪状态，这样的做，也极大的避免了cookie被篡改而带来的安全性问题 由于cookie可以被人为的禁止，必须有其他机制以便在cookie被禁止时仍然能够把session id传递回服务器。经常被使用的一种技术叫做URL重写，就是把session id直接附加在URL路径的后面，附加方式也有两种，一种是作为URL路径的附加信息，另一种是作为查询字符串附加在URL后面 "},"PHP/PHP安全处理机制.html":{"url":"PHP/PHP安全处理机制.html","title":"PHP安全处理机制","keywords":"","body":"问题列表 SQL 注入 主要就是一些数据没有经过严格的验证，然后直接拼接 SQL 去查询。导致漏洞产生,注入者可提交任何类型的数据，比如 \" and 1= 1 or \" 等不安全的数据 文件系统安全 PHP可以直接访问文件系统、执行Shell命令，这给程序开发提供了强大的支持的同时也可能会带来危险。同样，恰当的过滤和编码可以避免危险。 命令行注入 PHP提供了exec(), system(),passthru()，shell_exec()等函数，以及` (反引号)运算符。这些函数可以直接调用命令行系统指令,例如system('dir c:'); 可以显示C盘符下的目录内容, system(\"ls -al|cat/etc/passwd\");可以获得passwd的内容。假如攻击者能将这些命令注入你的代码并运行，将给系统带来巨大的危害。 XSS XSS 又叫 CSS (Cross Site Script) ，跨站脚本攻击。它指的是恶意攻击者往 Web 页面里插入恶意 html 代码，当用户浏览该页之时，嵌入其中 Web 里面的 html 代码会被执行，从而达到恶意攻击用户的特殊目的。 CSRF CSRF 是跨站请求伪造的缩写，它是攻击者通过一些技术手段欺骗用户去访问曾经认证过的网站并运行一些操作。 在生产环境中不正确的错误报告暴露敏感数据 如果你不小心，可能会在生产环境中因为不正确的错误报告泄露了敏感信息，例如：文件夹结构、数据库结构、连接信息与用户信息。 PHP预编译工作原理 预处理：创建 SQL 语句模板并发送到数据库。预留的值使用参数 \"?\" 标记 。例如： INSERT INTO MyGuests (firstname, lastname, email) VALUES(?, ?, ?) 数据库解析，编译，对SQL语句模板执行查询优化，并存储结果不输出。 执行：最后，将应用绑定的值传递给参数（\"?\" 标记），数据库执行语句。应用可以多次执行语句，如果参数的值不一样。 相比于直接执行SQL语句，预处理语句有两个主要优点： 预处理语句大大减少了分析时间，只做了一次查询（虽然语句多次执行）。 绑定参数减少了服务器带宽，你只需要发送查询的参数，而不是整个语句。 预处理语句针对SQL注入是非常有用的，因为参数值发送后使用不同的协议，保证了数据的合法性。 "},"PHP/require&include的区别.html":{"url":"PHP/require&include的区别.html","title":"require&include的区别","keywords":"","body":"处理文件错误的机制上面不同 require() :如果文件不存在，会报出一个fatal error.脚本停止执行; include() : 如果文件不存在，会给出一个 warning，但脚本会继续执行; php性能 对include()来说，在include()执行时文件每次都要进行读取和评估; 对require()来说，文件只处理一次(实际上，文件内容替换了require()语句)。 不同的使用弹性 require的使用方法如 require(\"./inc.php\"); 。通常放在PHP程式的最前面，PHP程式在执行前，就会先读入require所指定引入的档案，使它变成PHP 程式网页的一部份。 include使用方法如 include(\"./inc.php\"); 。一般是放在流程控制的处理区段中。PHP程式网页在读到 include的档案时，才将它读进来。这种方式，可以把程式执行时的流程简单化。 once 带once和不带once的区别主要是:带once的会判断你在加载这个文件之前是否已经加载过了文件，避免重复加载。 "},"PHP/PHP翻转数组.html":{"url":"PHP/PHP翻转数组.html","title":"PHP翻转数组","keywords":"","body":" array-reverse array-flip "},"PHP/秒杀系统的设计与实现.html":{"url":"PHP/秒杀系统的设计与实现.html","title":"秒杀系统的设计与实现","keywords":"","body":" 秒杀系统场景特点 [!TIP|label:正常电子商务流程] 查询商品 — 创建订单 — 扣减库存 — 更新订单 — 付款 — 卖家发货 秒杀时大量用户会在同一时间同时进行抢购，网站瞬时访问流量激增。 秒杀一般是访问请求数量远远大于库存数量，只有少部分用户能够秒杀成功。 秒杀业务流程比较简单，一般就是下订单减库存。 秒杀架构设计理念 限流： 鉴于只有少部分用户能够秒杀成功，所以要限制大部分流量，只允许少部分流量进入服务后端。 削峰：对于秒杀系统瞬时会有大量用户涌入，所以在抢购一开始会有很高的瞬间峰值。高峰值流量是压垮系统很重要的原因，所以如何把瞬间的高流量变成一段时间平稳的流量也是设计秒杀系统很重要的思路。实现削峰的常用的方法有利用缓存和消息中间件等技术。 异步处理：秒杀系统是一个高并发系统，采用异步处理模式可以极大地提高系统并发量，其实异步处理就是削峰的一种实现方式。 内存缓存：秒杀系统最大的瓶颈一般都是数据库读写，由于数据库读写属于磁盘IO，性能很低，如果能够把部分数据或业务逻辑转移到内存缓存，效率会有极大地提升。 可拓展：当然如果我们想支持更多用户，更大的并发，最好就将系统设计成弹性可拓展的，如果流量来了，拓展机器就好了。像淘宝、京东等双十一活动时会增加大量机器应对交易高峰。 设计思路 [!TIP|label:总结] 前端三板斧【扩容】【限流】【静态化】,后端两条路【内存】+【排队】 前段思路 页面静态化：将活动页面上的所有可以静态的元素全部静态化，并尽量减少动态元素。通过CDN来抗峰值。 禁止重复提交：用户提交之后按钮置灰，禁止重复提交 用户限流：在某一时间段内只允许用户提交一次请求，比如可以采取IP限流 控制层(网关层) 限制uid（UserID）访问频率：我们上面拦截了浏览器访问的请求，但针对某些恶意攻击或其它插件，在服务端控制层需要针对同一个访问uid，限制访问频率。 服务层 业务分离:将秒杀业务系统和其他业务分离，单独放在高配服务器上，可以集中资源对访问请求抗压。 采用消息队列缓存请求：将大流量请求写到消息队列缓存，利用服务器根据自己的处理能力主动到消息缓存队列中抓取任务处理请求，数据库层订阅消息减库存，减库存成功的请求返回秒杀成功，失败的返回秒杀结束。 利用缓存应对读请求：对于读多写少业务，大部分请求是查询请求，所以可以读写分离，利用缓存分担数据库压力。 利用缓存应对写请求：缓存也是可以应对写请求的，可把数据库中的库存数据转移到Redis缓存中，所有减库存操作都在Redis中进行，然后再通过后台进程把Redis中的用户秒杀请求同步到数据库中。 数据库层 数据库层是最脆弱的一层，一般在应用设计时在上游就需要把请求拦截掉，数据库层只承担“能力范围内”的访问请求。所以，上面通过在服务层引入队列和缓存，让最底层的数据库高枕无忧。 基于Redis秒杀&超卖问题解决 Redis事物 $redis = new redis(); $res = $redis->connect('127.0.0.1', 6379); if (!$res) { die('connect error!'); } $redis->select(6); //第一次取库存,先用保存到缓存中 $goods_id = 1; $stock_key = 'goods_id_stock_' . $goods_id; //先把库存取出来 $stock = $redis->get($stock_key); //监控key $redis->watch($stock_key); //开启事务 $redis->multi(); if ($stock === false) { echo 'set goodslist'; $info = $db->table('Goods')->get(1); $stock = $info['stock']; // 这个地方一定要用setnx,防止一开始并发的时候重复设置 $redis->setnx($stock_key, $info['stock']); } if ($stock > 0) { //加一些延时 sleep(1); $redis->decr($stock_key); //把这个产品信息添加到这个用户的订单中 //....一些逻辑代码 //提交事务 $res = $redis->exec(); if ($res === false) { //实际业务中上面如果有其它对数据库进行的操作这里要使用事务回滚 echo 'unknow'; } else { echo 'success'; } } else { echo 'fail'; } Redis队列 //1. 先将商品库存 存入队列 $redis = new Redis(); for($i=1;$ilpush('good','good_id'.$i); } print_r($redis->lrange('good',0,-1));exit; //2. 队列程序执行 header(\"content-type:text/html;charset=utf-8\"); $redis = new Redis(); //插入抢购数据 $userid = \"user_id_\".mt_rand(1, 9999).'_'.microtime(true); if($res = $redis->lpop('good')){ //$left = $redis->llen('good'); //剩余\".($left).\" $redis->lpush('good_res',$res); //file_put_contents('F:\\b.txt',$userid.\"抢购成功！\".$res.\"\\n\",FILE_APPEND); 写入文件可能会遇到并发锁 导致无法及时写入 而被直接跳过导致记录结果有误 建议测试使用mysql 或者 redis 存入日志记录 }else{ //file_put_contents('F:\\b.txt', $userid.\"手气不好，再抢购！\\n\",FILE_APPEND); } exit; //3. 打印执行结果 $redis = new Redis(); print_r($redis->lrange('good_res',0,-1));exit; 其他问题 对现有网站业务造成冲击 将秒杀系统独立部署，甚至使用独立域名，使其与网站完全隔离。 减库存的操作 有两种选择，一种是拍下减库存 另外一种是付款减库存；目前采用的“拍下减库存”的方式，拍下就是一瞬间的事，对用户体验会好些。 秒杀之后的支付完成，以及未支付取消占位，如何对剩余库存做及时的控制更新？ 数据库里一个状态，未支付。如果超过时间，例如45分钟，库存会重新会恢复（大家熟知的“回仓”），给我们抢票的启示是，开动秒杀后，45分钟之后再试试看，说不定又有票哟~ 减库存是在那个阶段减呢？如果是下单锁库存的话，大量恶意用户下单锁库存而不支付如何处理呢？ 数据库层面写请求量很低，还好，下单不支付，等时间过完再“回仓” 刷单处理 刷单的情况如下,写一个脚本不间断的发送请求,抢购页面的每次请求用这个用户的标识为一个redis缓存标记加一个过期时间,比如10秒内重复的请求不做处理，起到限流的作用 黄牛多买的情况,引导用户到付款成功时才根据活动规则减库存,限制购买个数 如果遇到全国大量黄牛集体刷单,比如淘宝双11,小米抢购,在优化购买流程已经到极限的情况下，可以加硬件配置,使用redis分布式架构配合nginx反向代理等方法进行分流引导 其他方案1 优点与说明 实现相对简单，能做一部分秒杀活动。 前端静态资源走CDN 前端做秒杀前后拦截，秒杀中，每个用户只放一次请求到后端。可以拦截掉小白用户的99%的无效点击。 利用redis的（setnx、incrby/decrby等原子操作）、mysql的（for update 写锁）实现秒杀库存控制缺点 比较倚重redis 过滤用户秒杀条件成为瓶颈 业务耦合还是比较严重 其他方案2 优点与说明 增加单机本地缓存，可以支持机器平行扩容，抗住更多请求 增加mq，做异步处理，解耦复杂的业务逻辑 缺点 没有机器人爬取。多个商品秒杀咋办？ 有没有什么降级处理？ 数据层还是单点 "},"PHP/PHP的垃圾回收机制.html":{"url":"PHP/PHP的垃圾回收机制.html","title":"PHP的垃圾回收机制","keywords":"","body":"引用计数 为每个内存对象分配一个计数器，当一个内存对象建立时计数器初始化为1（因此此时总是有一个变量引用此对象），以后每有一个新变量引用此内存对象，则计数器加1，而每当减少一个引用此内存对象的变量则计数器减1，当垃圾回收机制运作的时候，将所有计数器为0的内存对象销毁并回收其占用的内存。而PHP中内存对象就是zval，而计数器就是refcount__gc PHP5.3 Zval 每个zval表示一个变量的内存对象。除了value和type，可以看到_zval_struct中还有两个字段refcountgc和is_refgc，从其后缀就可以断定这两个家伙与垃圾回收有关。没错，PHP的垃圾回收全靠这俩字段了。其中refcountgc表示当前有几个变量引用此zval，而is_refgc表示当前zval是否被按引用引用，这话听起来很拗口，这和PHP中zval的“Write-On-Copy”机制有关 PHP5.3 回收机制 每个PHP的变量都存在于一个叫做zval的容器中,一个zval容器,除了包含变量名和值,还包括两个字节的额外信息,一个叫做'is_ref',是个布尔值,用来表示这个变量是否属于引用集合,通过这个字节,我们php才能把普通变量和引用变量区分开来.第二个额外字节就是'refcount',用来表示指向这个容器的变量的个数 首先PHP会分配一个固定大小的“根缓冲区”，这个缓冲区用于存放固定数量的zval，这个数量默认是10,000，如果需要修改则需要修改源代码Zend/zend_gc.c中的常量GC_ROOT_BUFFER_MAX_ENTRIES然后重新编译。 当根缓冲区满额时，PHP就会执行垃圾回收，此回收算法如下： 1、对每个根缓冲区中的根zval按照深度优先遍历算法遍历所有能遍历到的zval，并将每个zval的refcount减1，同时为了避免对同一zval多次减1（因为可能不同的根能遍历到同一个zval），每次对某个zval减1后就对其标记为“已减”。 2、再次对每个缓冲区中的根zval深度优先遍历，如果某个zval的refcount不为0，则对其加1，否则保持其为0。 3、清空根缓冲区中的所有根（注意是把这些zval从缓冲区中清除而不是销毁它们），然后销毁所有refcount为0的zval，并收回其内存。 如果不能完全理解也没有关系，只需记住PHP5.3的垃圾回收算法有以下几点特性： 1、并不是每次refcount减少时都进入回收周期，只有根缓冲区满额后在开始垃圾回收。 2、可以解决循环引用问题。 3、可以总将内存泄露保持在一个阈值以下。 PHP7 php7的垃圾回收包含两个部分，一个是垃圾收集器，一个是垃圾回收算法。 最基础的变化就是 zval 需要的内存 不再是单独从堆上分配，不再由 zval 存储引用计数。复杂数据类型（比如字符串、数组和对象）的引用计数由其自身来存储 php7的对于refcount的位置也已经从zval结构移到zval_value内了，所以不是所有的数据类型都会有引用计数操作，比如整型、浮点型、布尔、NULL这些简单数据类型都没有refcount了 垃圾回收只适用于array、object这两种类型。 变量的refcount减少到0，那么此变量可以被释放掉，不属于垃圾。 变量的refcount减少之后大于0，那么此变量还不能释放，此变量可能成为一个垃圾。 第一种情况就是可以理解为正常的unset操作，不属于垃圾。 第二种情况垃圾回收器才会将变量收集起来。 参考 垃圾回收是指当php运行状态结束时，比如遇到了exit/die/致命错误/脚本运行结束时，php需要回收运行过程中创建的变量、资源的内存。 ZEND引擎维护了一个栈zval，每个创建的变量和资源都会压入这个栈中，每个压入的数组结构都类似：[refcount => int, is_ref => 0|1, value => union, type => string]，变量被unset时，ref_count如果变成0，则被回收。 当遇到变量循环引用自身时，使用同步回收算法回收。 备注：PHP7已经重写了zal的结构体。 "},"缓存/Redis分布式锁的实现.html":{"url":"缓存/Redis分布式锁的实现.html","title":"Redis分布式锁的实现","keywords":"","body":"方案1 class Lock { private $redis=''; #存储redis对象 public function __construct($host,$port=6379) { $this->redis=new Redis(); $this->redis->connect($host,$port); } /** * @desc 加锁方法 * * @param $lockName string | 锁的名字 * @param $timeout int | 锁的过期时间 * * @return 成功返回identifier/失败返回false */ public function getLock($lockName, $timeout=2) { $identifier=uniqid(); #获取唯一标识符 $timeout=ceil($timeout); #确保是整数 $end=time()+$timeout; while(time()redis->setnx($lockName, $identifier)) #查看$lockName是否被上锁 { $this->redis->expire($lockName, $timeout); #为$lockName设置过期时间，防止死锁 return $identifier; #返回一维标识符 } elseif ($this->redis->ttl($lockName)===-1) {　　　　　　　　　　　　　　　　　　　　　　　　　　　　　 　 $this->redis->expire($lockName, $timeout); #检测是否有设置过期时间，没有则加上（假设，客户端A上一步没能设置时间就进程奔溃了，客户端B就可检测出来，并设置时间） } usleep(0.001); #停止0.001ms } return false; } /** * @desc 释放锁 * * @param $lockName string | 锁名 * @param $identifier string | 锁的唯一值 * * @param bool */ public function releaseLock($lockName,$identifier) { if($this->redis->get($lockName)==$identifier) #判断是锁有没有被其他客户端修改 { $this->redis->multi(); $this->redis->del($lockName); #释放锁 $this->redis->exec(); return true; } else { return false; #其他客户端修改了锁，不能删除别人的锁 } } /** * @desc 测试 * * @param $lockName string | 锁名 */ public function test($lockName) { $start=time(); for ($i=0; $i getLock($lockName); if($identifier) { $count=$this->redis->get('count'); $count=$count+1; $this->redis->set('count',$count); $this->releaseLock($lockName,$identifier); } } $end=time(); echo \"this OK\"; echo \"执行时间为：\".($end-$start); } } 方案2 class RedisMutexLock { /** * 缓存 Redis 连接。 * * @return void */ public static function getRedis() { // 这行代码请根据自己项目替换为自己的获取 Redis 连接。 return YCache::getRedisClient(); } /** * 获得锁,如果锁被占用,阻塞,直到获得锁或者超时。 * -- 1、如果 $timeout 参数为 0,则立即返回锁。 * -- 2、建议 timeout 设置为 0,避免 redis 因为阻塞导致性能下降。请根据实际需求进行设置。 * * @param string $key 缓存KEY。 * @param int $timeout 取锁超时时间。单位(秒)。等于0,如果当前锁被占用,则立即返回失败。如果大于0,则反复尝试获取锁直到达到该超时时间。 * @param int $lockSecond 锁定时间。单位(秒)。 * @param int $sleep 取锁间隔时间。单位(微秒)。当锁为占用状态时。每隔多久尝试去取锁。默认 0.1 秒一次取锁。 * @return bool 成功:true、失败:false */ public static function lock($key, $timeout = 0, $lockSecond = 20, $sleep = 100000) { if (strlen($key) === 0) { // 请更换为自己项目抛异常的方法。 YCore::exception(500, '缓存KEY没有设置'); } $start = self::getMicroTime(); $redis = self::getRedis(); do { // [1] 锁的 KEY 不存在时设置其值并把过期时间设置为指定的时间。锁的值并不重要。重要的是利用 Redis 的特性。 $acquired = $redis->set(\"Lock:{$key}\", 1, ['NX', 'EX' => $lockSecond]); if ($acquired) { break; } if ($timeout === 0) { break; } usleep($sleep); } while (!is_numeric($timeout) || (self::getMicroTime()) del(\"Lock:{$key}\"); } /** * 获取当前微秒。 * * @return bigint */ protected static function getMicroTime() { return bcmul(microtime(true), 1000000); } } "},"缓存/发送短信的缓存设计方案.html":{"url":"缓存/发送短信的缓存设计方案.html","title":"发送短信的缓存设计方案","keywords":"","body":"问题 一个手机号一个小时内只允许发送3条短信请给出缓存实现方案(考虑1000+的并发) 实现方式 "},"缓存/Redis的数据类型.html":{"url":"缓存/Redis的数据类型.html","title":"Redis的数据类型","keywords":"","body":" 类型 简介 特性 场景 String(字符串) 二进制安全 可以包含任何数据,比如jpg图片或者序列化的对象,一个键最大能存储512M --- Hash(字典) 键值对集合,即编程语言中的Map类型 适合存储对象,并且可以像数据库中update一个属性一样只修改某一项属性值(Memcached中需要取出整个字符串反序列化成对象修改完再序列化存回去) 存储、读取、修改用户属性 List(列表) 链表(双向链表) 增删快,提供了操作某一段元素的API 1,最新消息排行等功能(比如朋友圈的时间线) 2,消息队列 Set(集合) 哈希表实现,元素不重复 1、添加、删除,查找的复杂度都是O(1) 2、为集合提供了求交集、并集、差集等操作 1、共同好友 2、利用唯一性,统计访问网站的所有独立ip 3、好友推荐时,根据tag求交集,大于某个阈值就可以推荐 Sorted Set(有序集合) 将Set中的元素增加一个权重参数score,元素按score有序排列 数据插入集合时,已经进行天然排序 1、排行榜 2、带权重的消息队列 HyperLogLog(HYLL) 优点是，在输入元素的数量或者体积非常非常大时，计算基数所需的空间总是固定 的、并且是很小的。 每个 HYLL 键只需要花费 12 KB 内存，就可以计算接近 2^64 个不同元素的基 数。这和计算基数时，元素越多耗费内存就越多的集合形成鲜明对比。 用来做基数统计 误差为(0.81%) BitMap(位图) 对于bitmap，我们可以很容易的进行and、or操作，而且效率也是非常高的 对于bitmap的内存占用则是一个比较大的问题，它取决于基数的上限而非元素的个数。比如你的基数是1000W，那么你将需要分配1.2MB左右的内存空间给bitmap而不管你是否仅存了一个元素 可进行数据的快速查找，判重，删除 Sset命令 命令 描述 ZADD key score1 member1 [score2 member2] 向有序集合添加一个或多个成员，或者更新已存在成员的分数 ZCARD key 获取有序集合的成员数 ZCOUNT key min max 计算在有序集合中指定区间分数的成员数 ZINCRBY key increment member 有序集合中对指定成员的分数加上增量 increment ZINTERSTORE destination numkeys key [key ...] 计算给定的一个或多个有序集的交集并将结果集存储在新的有序集合 key 中 ZLEXCOUNT key min max 在有序集合中计算指定字典区间内成员数量 ZRANGE key start stop [WITHSCORES] 通过索引区间返回有序集合成指定区间内的成员 ZRANGEBYLEX key min max [LIMIT offset count] 通过字典区间返回有序集合的成员 ZRANGEBYSCORE key min max [WITHSCORES] [LIMIT] 通过分数返回有序集合指定区间内的成员 ZRANK key member 返回有序集合中指定成员的索引 ZREM key member [member ...] 移除有序集合中的一个或多个成员 ZREMRANGEBYLEX key min max 移除有序集合中给定的字典区间的所有成员 ZREMRANGEBYRANK key start stop 移除有序集合中给定的排名区间的所有成员 ZREMRANGEBYSCORE key min max 移除有序集合中给定的分数区间的所有成员 ZREVRANGE key start stop [WITHSCORES] 返回有序集中指定区间内的成员，通过索引，分数从高到底 ZREVRANGEBYSCORE key max min [WITHSCORES] 返回有序集中指定分数区间内的成员，分数从高到低排序 ZREVRANK key member 返回有序集合中指定成员的排名，有序集成员按分数值递减(从大到小)排序 ZSCORE key member 返回有序集中，成员的分数值 ZUNIONSTORE destination numkeys key [key ...] 计算给定的一个或多个有序集的并集，并存储在新的 key 中 ZSCAN key cursor [MATCH pattern] [COUNT count] 迭代有序集合中的元素（包括元素成员和元素分值） Set命令 命令 描述 SADD key member1 [member2] 向集合添加一个或多个成员 SCARD key 获取集合的成员数 SDIFF key1 [key2] 返回给定所有集合的差集 SDIFFSTORE destination key1 [key2] 返回给定所有集合的差集并存储在 destination 中 SINTER key1 [key2] 返回给定所有集合的交集 SINTERSTORE destination key1 [key2] 返回给定所有集合的交集并存储在 destination 中 SISMEMBER key member 判断 member 元素是否是集合 key 的成员 SMEMBERS key 返回集合中的所有成员 SMOVE source destination member 将 member 元素从 source 集合移动到 destination 集合 SPOP key 移除并返回集合中的一个随机元素 SRANDMEMBER key [count] 返回集合中一个或多个随机数 SREM key member1 [member2] 移除集合中一个或多个成员 SUNION key1 [key2] 返回所有给定集合的并集 SUNIONSTORE destination key1 [key2] 所有给定集合的并集存储在 destination 集合中 SSCAN key cursor [MATCH pattern] [COUNT count] 迭代集合中的元素 List命令 命令 描述 BLPOP key1 [key2 ] timeout 移出并获取列表的第一个元素， 如果列表没有元素会阻塞列表直到等待超时或发现可弹出元素为止。 BRPOP key1 [key2 ] timeout 移出并获取列表的最后一个元素， 如果列表没有元素会阻塞列表直到等待超时或发现可弹出元素为止。 BRPOPLPUSH source destination timeout 从列表中弹出一个值，将弹出的元素插入到另外一个列表中并返回它； 如果列表没有元素会阻塞列表直到等待超时或发现可弹出元素为止。 LINDEX key index 通过索引获取列表中的元素 LINSERT key BEFORE 或 AFTER pivot value 在列表的元素前或者后插入元素 LLEN key 获取列表长度 LPOP key 移出并获取列表的第一个元素 LPUSH key value1 [value2] 将一个或多个值插入到列表头部 LPUSHX key value 将一个值插入到已存在的列表头部 LRANGE key start stop 获取列表指定范围内的元素 LREM key count value 移除列表元素 LSET key index value 通过索引设置列表元素的值 LTRIM key start stop 对一个列表进行修剪(trim)，就是说，让列表只保留指定区间内的元素，不在指定区间之内的元素都将被删除。 RPOP key 移除列表的最后一个元素，返回值为移除的元素。 RPOPLPUSH source destination 移除列表的最后一个元素，并将该元素添加到另一个列表并返回 RPUSH key value1 [value2] 在列表中添加一个或多个值 RPUSHX key value 为已存在的列表添加值 Hash命令 命令 描述 HDEL key field1 [field2] 删除一个或多个哈希表字段 HEXISTS key field 查看哈希表 key 中，指定的字段是否存在。 HGET key field 获取存储在哈希表中指定字段的值。 HGETALL key 获取在哈希表中指定 key 的所有字段和值 HINCRBY key field increment 为哈希表 key 中的指定字段的整数值加上增量 increment 。 HINCRBYFLOAT key field increment 为哈希表 key 中的指定字段的浮点数值加上增量 increment 。 HKEYS key 获取所有哈希表中的字段 HLEN key 获取哈希表中字段的数量 HMGET key field1 [field2] 获取所有给定字段的值 HMSET key field1 value1 [field2 value2 ] 同时将多个 field-value (域-值)对设置到哈希表 key 中。 HSET key field value 将哈希表 key 中的字段 field 的值设为 value 。 HSETNX key field value 只有在字段 field 不存在时，设置哈希表字段的值。 HVALS key 获取哈希表中所有值 HSCAN key cursor [MATCH pattern] [COUNT count] 迭代哈希表中的键值对。 "},"缓存/关于Redis的原子性.html":{"url":"缓存/关于Redis的原子性.html","title":"关于Redis的原子性","keywords":"","body":"Redis的原子性？什么是原子操作？ [!TIP|label:原子性(atomicity)] 一个事务是一个不可分割的最小工作单位,要么都成功要么都失败。(原子操作是指你的一个业务逻辑必须是不可拆分的.比如你给别人转钱,你的账号扣钱,别人的账号增加钱,这个业务逻辑就是原子性的,这个操作就是原子操作,要么都成功要么都失败) 对于Redis而言，命令的原子性指的是：一个操作的不可以再分，操作要么执行，要么不执行。 Redis所有单个命令的执行都是原子性的。 多个操作也支持事务，即原子性，通过MULTI和EXEC指令包起来 Redis是单进程单线程的网络模型，用的是epoll网络模型，网络模型都是单线程异步非阻塞处理网络请求 Redis的单线程处理所有的客户端连接请求，命令读写请求。（有些任务比如rdb和aof等操作是fork子进程处理的，不会影响redis主线程处理客户端的命令） Redis提供的所有API操作，相对于服务端方面都是one by one执行的，命令是一个接着一个执行的，不存在并行执行的情况。 Redis的操作之所以是原子性的，是因为Redis是单线程的(操作系统能够进行运算调度的最小单元。它被包含在进程之中，是进程的实际运作单位。一条线程指的是进程中一个单一顺序的控制流，一个进程中可以并发多个线程，每条线程并行执行不同的任务) redis的事务 MULTI 、 EXEC 、 DISCARD 和 WATCH 是 Redis 事务相关的命令。事务可以一次执行多个命令， 并且带有以下两个重要的保证 事务是一个单独的隔离操作：事务中的所有命令都会序列化、按顺序地执行。事务在执行的过程中，不会被其他客户端发送来的命令请求所打断。 事务是一个原子操作：事务中的命令要么全部被执行，要么全部都不执行 "},"缓存/Redis的持久化处理.html":{"url":"缓存/Redis的持久化处理.html","title":"Redis的持久化处理","keywords":"","body":"RDB持久化（快照） [!TIP|label:说明] RDB持久化是指在客户端输入save、bgsave或者达到配置文件自动保存快照条件时，将Redis 在内存中的数据生成快照保存在名字为 dump.rdb（文件名可修改）的二进制文件中。 save命令 save命令会阻塞Redis服务器进程，直到RDB文件创建完毕为止，在Redis服务器阻塞期间，服务器不能处理任何命令请求。 在客户端输入save bgsave命令 该触发方式会fork一个子进程，由子进程负责持久化过程，因此阻塞只会发生在fork子进程的时候。 服务器进程pid为1349派生出一个pid为1357的子进程， 子进程将数据写入到一个临时 RDB 文件中 当子进程完成对新 RDB 文件的写入时，Redis 用新 RDB 文件替换原来的 RDB 文件，并删除旧的 RDB 文件。 bgsave命令执行期间,SAVE命令会被拒绝,不能同时执行两个BGSAVE命令,不能同时执行BGREWRITEAOF和BGSAVE命令 原理说明 这里注意的是 fork 操作会阻塞，导致Redis读写性能下降。我们可以控制单个Redis实例的最大内存，来尽可能降低Redis在fork时的事件消耗。以及上面提到的自动触发的频率减少fork次数，或者使用手动触发，根据自己的机制来完成持久化。 优点 RDB是一个非常紧凑（有压缩）的文件,它保存了某个时间点的数据,非常适用于数据备份&灾难恢复. RDB在保存RDB文件时父进程唯一需要做的就是fork出一个子进程,接下来的工作全部由子进程来做，父进程不需要再做其他IO操作，所以RDB持久化方式可以最大化redis的性能. 与AOF相比,在恢复大的数据集的时候，RDB方式会更快一些. 缺点 Redis意外宕机 时，会丢失部分数据 当Redis数据量比较大时，fork的过程是非常耗时的，fork子进程时是会阻塞的，在这期间Redis 是不能响应客户端的请求的。 自动触发的场景 根据我们的 save m n 配置规则自动触发； 从节点全量复制时，主节点发送rdb文件给从节点完成复制操作，主节点会触发 bgsave； 执行 debug reload 时； 执行 shutdown时，如果没有开启aof，也会触发。 配置说明 # 触发自动保存快照 # save # save # save 900 1 表示900s内如果有1条是写入命令，就触发产生一次快照，可以理解为就进行一次备份 # save 300 10 表示300s内有10条写入，就产生快照 # 当然如果你想要禁用RDB配置，也是非常容易的，只需要在save的最后一行写上：save \"\" save 900 1 save 300 10 save 60 10000 # 设置在保存快照出错时，是否停止redis命令的写入,这个配置也是非常重要的一项配置，这是当备份进程出错时，主进程就停止接受新的写入操作，是为了保护持久化的数据一致性问题 stop-writes-on-bgsave-error yes # 是否在导出.rdb数据库文件的时候采用LZF压缩,建议没有必要开启，毕竟Redis本身就属于CPU密集型服务器，再开启压缩会带来更多的CPU消耗，相比硬盘成本，CPU更值钱。 rdbcompression yes # 是否开启CRC64校验 rdbchecksum yes # 导出数据库的文件名称 dbfilename dump.rdb # 导出的数据库所在的目录 dir ./ AOF持久化（只追加操作的文件 Append-only file） [!TIP|label:说明] AOF持久化是通过保存Redis服务器所执行的写命令来记录数据库状态，也就是每当 Redis 执行一个改变数据集的命令时（比如 SET）， 这个命令就会被追加到 AOF 文件的末尾。 原理说明 在重写期间，由于主进程依然在响应命令，为了保证最终备份的完整性；因此它依然会写入旧的AOF file中，如果重写失败，能够保证数据不丢失。 为了把重写期间响应的写入信息也写入到新的文件中，因此也会为子进程保留一个buf，防止新写的file丢失数据。 重写是直接把当前内存的数据生成对应命令，并不需要读取老的AOF文件进行分析、命令合并。 AOF文件直接采用的文本协议，主要是兼容性好、追加方便、可读性高可认为修改修复。 优点 AOF文件是一个只进行追加的日志文件，不需要在写入时读取文件。 Redis 可以在 AOF 文件体积变得过大时，自动地在后台对 AOF 进行重写 。 AOF文件可读性高，分析容易 缺点 对于相同的数据来说，AOF 文件大小通常要大于 RDB 文件 根据所使用的 fsync 策略，AOF 的速度可能会慢于 RDB 重写 由于AOF 持久化是通过不断地将命令追加到文件的末尾来记录数据库状态的， 所以随着写入命令的不断增加， AOF 文件的体积也会变得越来越大。 且有些命令是改变同一数据，是可以合并成一条命令的。就好比对一个计数器调用了 100 次 INCR，AOF就会存入100 条记录，其实存入一条数据就可以了。(AOF重写机制的触发有两种机制，一个是通过调用命令BGREWRITEAOF,另一种是根据配置文件中的参数触发) 重写步骤 创建子进程进行AOF重写 将客户端的写命令追加到AOF重写缓冲区 子进程完成AOF重写工作后，会向父进程发送一个信号 父进程接收到信号后，将AOF重写缓冲区的所有内容写入到新AOF文件中 对新的AOF文件进行改名，原子的覆盖现有的AOF文件 注：AOF重写不需要对现有的AOF文件进行任何读取、分析和写入操作。 配置说明 # 是否开启AOF功能 appendonly no # AOF文件件名称 appendfilename \"appendonly.aof\" # 写入AOF文件的三种方式 # appendfsync always (把每个写命令都立即同步到aof，很慢，但是很安全) appendfsync everysec # (每秒同步一次，是折中方案,一般情况下都采用 everysec 配置，这样可以兼顾速度与安全，最多损失1s的数据。) # appendfsync no (edis不处理交给OS来处理，非常快，但是也最不安全) # 重写AOF时，是否继续写AOF文件 no-appendfsync-on-rewrite no # 自动重写AOF文件的条件 auto-aof-rewrite-percentage 100 #百分比 auto-aof-rewrite-min-size 64mb #大小 # 是否忽略最后一条可能存在问题的指令,如果该配置启用，在加载时发现aof尾部不正确是，会向客户端写入一个log，但是会继续执行，如果设置为 no ，发现错误就会停止，必须修复后才能重新加载。 aof-load-truncated yes # 文件重写策略 aof-rewrite-incremental-fsync yes 性能与实践 通过上面的分析，我们都知道RDB的快照、AOF的重写都需要fork，这是一个重量级操作，会对Redis造成阻塞。因此为了不影响Redis主进程响应，我们需要尽可能降低阻塞。 降低fork的频率，比如可以手动来触发RDB生成快照、与AOF重写； 控制Redis最大使用内存，防止fork耗时过长； 使用更牛逼的硬件； 合理配置Linux的内存分配策略，避免因为物理内存不足导致fork失败。 如果Redis中的数据并不是特别敏感或者可以通过其它方式重写生成数据，可以关闭持久化，如果丢失数据可以通过其它途径补回； 自己制定策略定期检查Redis的情况，然后可以手动触发备份、重写数据； 单机如果部署多个实例，要防止多个机器同时运行持久化、重写操作，防止出现内存、CPU、IO资源竞争，让持久化变为串行； 可以加入主从机器，利用一台从机器进行备份处理，其它机器正常响应客户端的命令； RDB持久化与AOF持久化可以同时存在，配合使用。 从持久化中恢复数据 启动时会先检查AOF文件是否存在，如果不存在就尝试加载RDB。那么为什么会优先加载AOF呢？因为AOF保存的数据更完整，通过上面的分析我们知道AOF基本上最多损失1s的数据。 "},"缓存/Redis过期键的删除策略.html":{"url":"缓存/Redis过期键的删除策略.html","title":"Redis过期键的删除策略","keywords":"","body":"当内存达到maxmemory配置时候，触发主动清理策略 当前已用内存超过maxmemory限定时，触发主动清理策略，这些策略可以配置（参数maxmemory-policy） 尽量不要触发maxmemory，最好在mem_used内存占用达到maxmemory的一定比例后，需要考虑调大hz以加快淘汰，或者进行集群扩容。 当mem_used内存已经超过maxmemory的设定，对于所有的读写请求，都会触发redis.c/freeMemoryIfNeeded(void)函数以清理超出的内存。注意这个清理过程是阻塞的，直到清理出足够的内存空间。所以如果在达到maxmemory并且调用方还在不断写入的情况下，可能会反复触发主动清理策略，导致请求会有一定的延迟。 清理时会根据用户配置的maxmemory-policy来做适当的清理（一般是LRU或TTL），这里的LRU或TTL策略并不是针对redis的所有key，而是以配置文件中的maxmemory-samples个key作为样本池进行抽样清理。策略列表 volatile-lru：从已设置过期时间的数据集（server.db[i].expires）中挑选最近最少使用的数据淘汰 volatile-ttl：从已设置过期时间的数据集（server.db[i].expires）中挑选将要过期的数据淘汰 volatile-random：从已设置过期时间的数据集（server.db[i].expires）中任意选择数据淘汰 allkeys-lru：从数据集（server.db[i].dict）中挑选最近最少使用的数据淘汰 allkeys-random：从数据集（server.db[i].dict）中任意选择数据淘汰 no-enviction（驱逐）：禁止驱逐数据 被动删除（惰性删除） 当读/写一个已经过期的key时，会触发惰性删除策略，直接删除掉这个过期key 只有key被操作时(如GET)，REDIS才会被动检查该key是否过期，如果过期则删除之并且返回NIL。 这种删除策略对CPU是友好的，删除操作只有在不得不的情况下才会进行，不会其他的expire key上浪费无谓的CPU时间。 但是这种策略对内存不友好，一个key已经过期，但是在它被操作之前不会被删除，仍然占据内存空间。如果有大量的过期键存在但是又很少被访问到，那会造成大量的内存空间浪费。expireIfNeeded(redisDb db, robj key)函数位于src/db.c。 但仅是这样是不够的，因为可能存在一些key永远不会被再次访问到，这些设置了过期时间的key也是需要在过期后被删除的，我们甚至可以将这种情况看作是一种内存泄露----无用的垃圾数据占用了大量的内存，而服务器却不会自己去释放它们，这对于运行状态非常依赖于内存的Redis服务器来说，肯定不是一个好消息 主动删除（定期删除） 由于惰性删除策略无法保证冷数据被及时删掉，所以Redis会定期主动淘汰一批已过期的key 先说一下时间事件，对于持续运行的服务器来说， 服务器需要定期对自身的资源和状态进行必要的检查和整理， 从而让服务器维持在一个健康稳定的状态， 这类操作被统称为常规操作（cron job） 在 Redis 中， 常规操作由 redis.c/serverCron 实现， 它主要执行以下操作 更新服务器的各类统计信息，比如时间、内存占用、数据库占用情况等。 清理数据库中的过期键值对。 对不合理的数据库进行大小调整。 关闭和清理连接失效的客户端。 尝试进行 AOF 或 RDB 持久化操作。 如果服务器是主节点的话，对附属节点进行定期同步。 如果处于集群模式的话，对集群进行定期同步和连接测试。 Redis 将 serverCron 作为时间事件来运行， 从而确保它每隔一段时间就会自动运行一次， 又因为 serverCron 需要在 Redis 服务器运行期间一直定期运行， 所以它是一个循环时间事件： serverCron 会一直定期执行，直到服务器关闭为止。 在 Redis 2.6 版本中， 程序规定 serverCron 每秒运行 10 次， 平均每 100 毫秒运行一次。 从 Redis 2.8 开始， 用户可以通过修改 hz选项来调整 serverCron 的每秒执行次数， 具体信息请参考 redis.conf 文件中关于 hz 选项的说明 也叫定时删除，这里的“定期”指的是Redis定期触发的清理策略，由位于src/redis.c的activeExpireCycle(void)函数来完成。 serverCron是由redis的事件框架驱动的定位任务，这个定时任务中会调用activeExpireCycle函数，针对每个db在限制的时间REDIS_EXPIRELOOKUPS_TIME_LIMIT内迟可能多的删除过期key，之所以要限制时间是为了防止过长时间 的阻塞影响redis的正常运行。这种主动删除策略弥补了被动删除策略在内存上的不友好。 因此，Redis会周期性的随机测试一批设置了过期时间的key并进行处理。测试到的已过期的key将被删除。典型的方式为,Redis每秒做10次如下的步骤： 随机测试100个设置了过期时间的key 删除所有发现的已过期的key 若删除的key超过25个则重复步骤1 这是一个基于概率的简单算法，基本的假设是抽出的样本能够代表整个key空间，redis持续清理过期的数据直至将要过期的key的百分比降到了25%以下。这也意味着在任何给定的时刻已经过期但仍占据着内存空间的key的量最多为每秒的写操作量除以4. Redis-3.0.0中的默认值是10，代表每秒钟调用10次后台任务。 除了主动淘汰的频率外，Redis对每次淘汰任务执行的最大时长也有一个限定，这样保证了每次主动淘汰不会过多阻塞应用请求 hz调大将会提高Redis主动淘汰的频率，如果你的Redis存储中包含很多冷数据占用内存过大的话，可以考虑将这个值调大，但Redis作者建议这个值不要超过100。我们实际线上将这个值调大到100，观察到CPU会增加2%左右，但对冷数据的内存释放速度确实有明显的提高（通过观察keyspace个数和used_memory大小）。 可以看出timelimit和server.hz是一个倒数的关系，也就是说hz配置越大，timelimit就越小。换句话说是每秒钟期望的主动淘汰频率越高，则每次淘汰最长占用时间就越短。这里每秒钟的最长淘汰占用时间是固定的250ms（1000000*ACTIVE_EXPIRE_CYCLE_SLOW_TIME_PERC/100），而淘汰频率和每次淘汰的最长时间是通过hz参数控制的。 从以上的分析看，当redis中的过期key比率没有超过25%之前，提高hz可以明显提高扫描key的最小个数。假设hz为10，则一秒内最少扫描200个key（一秒调用10次*每次最少随机取出20个key），如果hz改为100，则一秒内最少扫描2000个key；另一方面，如果过期key比率超过25%，则扫描key的个数无上限，但是cpu时间每秒钟最多占用250ms。 当REDIS运行在主从模式时，只有主结点才会执行上述这两种过期删除策略，然后把删除操作”del key”同步到从结点。 "},"消息队列/RabbitMQ消息一致性怎么保证.html":{"url":"消息队列/RabbitMQ消息一致性怎么保证.html","title":"RabbitMQ消息一致性怎么保证","keywords":"","body":"生产者丢数据 确认机制相对于事务机制，最大的好处就是可以异步处理提高吞吐量，不需要额外等待消耗资源。但是两者时候不能同时共存的。 事务机制 [!TIP|label:说明] 与事务机制相关的有三种方法，分别是channel.txSelect设置当前信道为事务模式、channel.txCommit提交事务和channel.txRollback事务回滚。如果事务提交成功，则消息一定是到达了RabbitMQ中，如果事务提交之前由于发送异常或者其他原因，捕获后可以进行channel.txRollback回滚。 // 将信道设置为事务模式，开启事务 channel.txSelect(); // 发送持久化消息 channel.basicPublish(EXCHANGE_NAME, ROUTING_KEY, MessageProperties.PERSISTENT_TEXT_PLAIN, \"transaction messages\".getBytes()); // 事务提交 channel.txCommit(); // 事物回滚 channel.txRollback(); 消息确认机制 [!TIP|label:说明] 生产者将信道设置为confirm确认模式，确认之后所有在信道上的消息将会被指派一个唯一的从1开始的ID，一旦消息被正确匹配到所有队列后，RabbitMQ就会发送一个确认Basic.Ack给生产者（包含消息的唯一ID），生产者便知晓消息是否正确到达目的地了。 消息如果是持久化的，那么确认消息会在消息写入磁盘之后发出。RabbitMQ中的deliveryTag包含了确认消息序号，还可以设置multiple参数，表示到这个序号之前的所有消息都已经得到处理。确认机制相对事务机制来说，相比较代码来说比较复杂，但会经常使用，主要有单条确认、批量确认、异步批量确认三种方式。 单条确认 // 设置 channel 消息推送为 单条确认模式 channel.confirmSelect(); // 推送消息 channel.basicPublish(\"exchange\", \"routingkey\", null, \"publisher confirm test\".getBytes()); // 消息推送失败处理 if (!channel.waitForConfirms()) { //publisher confirm failed handle } 批量确认 [!TIP|label:问题] 批量确认comfirm需要解决出现返回的Basic.Nack或者超时情况的消息全部重发怎么解决 # 增加一个缓存，将发送成功并且确认Ack之后的消息去除，剩下Nack或者超时的消息 // take ArrayList or BlockingQueue as a cache List cache = new ArrayList<>(); // set channel publisher confirm mode channel.confirmSelect(); for (int i=0; i 异步批量确认 [!NOTE|lable:说明] 异步确认方式通过在客户端addConfirmListener增加ConfirmListener回调接口，包括handleAck与handleNack处理方法 // take as a cache SortedSet cache = new TreeSet(); // set channel publisher confirm mode channel.confirmSelect(); for (int i = 0; i 参考 四种途径提高RabbitMQ传输消息数据的可靠性（一） 四种途径提高RabbitMQ传输数据的可靠性（二） RabbitMQ如何解决各种情况下丢数据的问题 "},"消息队列/RabbitMQ是如何运转的.html":{"url":"消息队列/RabbitMQ是如何运转的.html","title":"RabbitMQ是如何运转的","keywords":"","body":"MQ的作用 解耦：可以很好地屏蔽应用程序及平台之间的特性，充当中间者，松散耦合应用程序及平台，它们彼此不需要了解远程过程调用RPC与网络协议的细节； 异步通信：能提供C/S之间同步与异步连接，在任何时刻都可以将消息进行传送或者存储转发； 可恢复性：当消息接收方宕机或网络不通的情况下，消息转储于MQ中，直到网络恢复或接收方恢复再进行转发； 扩展性：提高消息入队列和处理效率是容易的，只需要另外增加处理过程即可，不需要改变代码，也不需要调节参数。 顺序性：由于大部分MQ支持队列模式，自然也就能保证一定的数据处理顺序（事实上是不准确的，很有局限性。消息的顺序性取决于很多因素） 缓冲：MQ通过一个缓冲层来帮助任务最高效率执行，写入MQ的处理会尽可能快速。 RabbitMQ的消息处理过程 RabbitMQ的四种交换机 直连交换机：Direct exchange [!TIP|label:适用场景] 有优先级的任务，根据任务的优先级把消息发送到对应的队列，这样可以指派更多的资源去处理高优先级的队列。 直连交换机是一种带路由功能的交换机，一个队列会和一个交换机绑定，除此之外再绑定一个routing_key，当消息被发送的时候，需要指定一个binding_key，这个消息被送达交换机的时候，就会被这个交换机送到指定的队列里面去。同样的一个binding_key也是支持应用到多个队列中的。这样当一个交换机绑定多个队列，就会被送到对应的队列去处理。 扇形交换机：Fanout exchange 广播消息。扇形交换机会把能接收到的消息全部发送给绑定在自己身上的队列。因为广播不需要“思考”，所以扇形交换机处理消息的速度也是所有的交换机类型里面最快的。 主题交换机：Topic exchange RabbitMQ提供了一种主题交换机，发送到主题交换机上的消息需要携带指定规则的routing_key，主题交换机会根据这个规则将数据发送到对应的(多个)队列上。 主题交换机的routing_key需要有一定的规则，交换机和队列的binding_key需要采用.#......的格式，每个部分用.分开 *表示一个单词 表示任意数量（零个或多个）单词。 首部交换机：Headers exchange 首部交换机是忽略routing_key的一种路由方式。路由器和交换机路由的规则是通过Headers信息来交换的，这个有点像HTTP的Headers。将一个交换机声明成首部交换机，绑定一个队列的时候，定义一个Hash的数据结构，消息发送的时候，会携带一组hash数据结构的信息，当Hash的内容匹配上的时候，消息就会被写入队列。 绑定交换机和队列的时候，Hash结构中要求携带一个键“x-match”，这个键的Value可以是any或者all，这代表消息携带的Hash是需要全部匹配(all)，还是仅匹配一个键(any)就可以了。相比直连交换机，首部交换机的优势是匹配的规则不被限定为字符串(string)。 生产者流程　　 生产者连接到RabbitMQ Broker，建立Connection，开启信道Channel（Connection与Channel概念下面会介绍） 生产者声明一个交换器，设置相关属性。 生产者声明一个队列并设置相关属性 生产者通过路由键将交换器和队列绑定起来 生产者发送消息到RabbitMQ Broker，包括路由键、交换器信息等 相应的交换器根据路由键查找匹配的队列 如果找到则消息存入相应队列中 如果没找到则根据配置的属性丢弃或者回退给生产者 关闭信道 关闭连接 消费者流程 消费者连接到RabbitMQ Broker，建立Connection，开启Channel 消费者向RabbitMQ Broker请求消费相应队列中消息，可能会设置相应的回调函数。 等待RabbitMQ Broker回应并投递相应队列中的消息，消费者接收消息。 消费者确认ack接收到的消息。 RabbitMQ从队列中删除相应已经被确认的消息。 关闭信道。 关闭连接 Connection与Channel概念 Connection：实际就是一条TCP连接，TCP一旦建立起来，客户端紧接着可以创建AMQP信道。 Channel：每个Channel都有唯一的ID，都是建立在Connection上的虚拟连接，RabbitMQ处理每条AMQP指令都是通过信道完成的 [!TIP|label:单TCP复用连接与多信道的优势] 为什么TCP连接只有一条，而每个生产者都会创建一条唯一的信道呢？想象下，实际情况，会有很多的生产者生产消息，多个消费者消费消息，那么就不得不创建多个线程，建立多个TCP连接。多个TCP连接的建立必然会对操作系统性能消耗较高，也不方便管理。从而选择一种类似于NIO（非阻塞I/O, Non-blocking I/O）技术是很有必要的，多信道的在TCP基础上的建立就是这么实现的。 每个线程都有自己的一个信道，复用了Connection的TCP连接，信道之间相互独立，相互保持神秘，节约TCP连接资源，当然本身信道的流量很大的话，也可以创建多个适当的Connection的TCP连接，需要根据具体业务情况制定。 参考 RabbitMQ的四种交换机 RabbitMQ是如何运转的？ 认识RabbitMQ交换机模型 "},"消息队列/RabbitMQ集群实现方式.html":{"url":"消息队列/RabbitMQ集群实现方式.html","title":"RabbitMQ集群实现方式","keywords":"","body":"RabbitMQ集群实现方式 主备模式(cluster) 不支持跨网段，用于同一个网段内的局域网 可以随意的动态增加或者减少 节点之间需要运行相同版本的RabbitMQ和Erlang 也称为 Warren (兔子窝) 模式。实现 rabbitMQ 的高可用集群，一般在并发和数据量不高的情况下，这种模式非常的好用且简单。也就是一个主/备方案，主节点提供读写，备用节点不提供读写。如果主节点挂了，就切换到备用节点，原来的备用节点升级为主节点提供读写服务，当原来的主节点恢复运行后，原来的主节点就变成备用节点，和 activeMQ 利用 zookeeper 做主/备一样，也可以一主多备。 镜像模式(mirror) RabbitMQ的Cluster集群模式一般分为两种，普通模式和镜像模式。 将需要消费的队列变为镜像队列，存在于多个节点，这样就可以实现RabbitMQ的HA高可用性。作用就是消息实体会主动在镜像节点之间实现同步，而不是像普通模式那样，在consumer消费数据时临时读取。缺点就是，集群内部的同步通讯会占用大量的网络带宽。 RabbitMQ主从之间的数据复制是异步的，但是在rabbitmq中不会出现mysql那种丢数据的情况，这是因为rabbitmq的接口也是异步的，主收到一条消息写入本地存储，然后在发起写入从的请求。当所有从写入成功后，主才会给client返回ack说这次写入成功了。所以可以看出，虽然rabbitmq的主从复制是异步的，但是并且不会出现mysql丢数据的场景。只要客户端收到ack，就说明这条消息已经写入主和从了。 多活模式(federation) 应用于广域网，允许单台服务器上的交换机或队列接收发布到另一台服务器上交换机或队列的消息，可以是单独机器或集群。federation队列类似于单向点对点连接，消息会在联盟队列之间转发任意次，直到被消费者接受。通常使用federation来连接internet上的中间服务器，用作订阅分发消息或工作队列。 federation 插件是一个不需要构建 cluster ，而在 brokers 之间传输消息的高性能插件，federation 插件可以在 brokers 或者 cluster 之间传输消息，连接的双方可以使用不同的 users 和 virtual hosts，双方也可以使用不同版本的 rabbitMQ 和 erlang。federation 插件使用 AMQP 协议通信，可以接受不连续的传输。federation 不是建立在集群上的，而是建立在单个节点上的 远程模式(shovel) 连接方式与federation的连接方式类似，但它工作在更低层次。可以应用于广域网。 远程模式可以实现双活的一种模式，简称 shovel 模式，所谓的 shovel 就是把消息进行不同数据中心的复制工作，可以跨地域的让两个 MQ 集群互联，远距离通信和复制。 Shovel 就是我们可以把消息进行数据中心的复制工作，我们可以跨地域的让两个 MQ 集群互联。 参考 RabbitMQ 的4种集群架构 RabbitMQ高可用原理 RabbitMQ主备复制是异步还是同步？ RabbitMQ集群搭建-镜像模式 "},"算法实现/常见算法时间复杂度.html":{"url":"算法实现/常见算法时间复杂度.html","title":"常见算法时间复杂度","keywords":"","body":"优劣情况 [!TIP|label:时间复杂度按数量级递增排列顺序] 常数阶O(1)、对数阶O(log2n)、线性阶O(n)、线性对数阶O(nlog2n)、平方阶O(n2)、立方阶O(n3)、……k次方阶O(nk)、指数阶O(2n)。 优常见列表 排序法 最差时间分析 平均时间复杂度 稳定度 空间复杂度 冒泡排序 O(n2) O(n2) 稳定 O(1) 快速排序 O(n2) O(n*log2n) 不稳定 O(log2n)~O(n) 选择排序 O(n2) O(n2) 稳定 O(1) 二叉树排序 O(n2) O(n*log2n) 不一顶 O(n) 插入排序 O(n2) O(n2) 稳定 O(1) 堆排序 O(n*log2n) O(n*log2n) 不稳定 O(1) 希尔排序 O O 不稳定 O(1) "},"算法实现/冒泡排序.html":{"url":"算法实现/冒泡排序.html","title":"冒泡排序","keywords":"","body":"$demo_array = array(23,15,43,25,54,2,6,82,11,5,21,32,65); // 第一层for循环可以理解为从数组中键为0开始循环到最后一个 for ($i=0;$i $demo_array[$j]) { $tmp = $demo_array[$i]; // 这里的tmp是临时变量 $demo_array[$i] = $demo_array[$j]; // 第一次更换位置 $demo_array[$j] = $tmp; // 完成位置互换 } } } "},"算法实现/快速排序.html":{"url":"算法实现/快速排序.html","title":"快速排序","keywords":"","body":"function quick_sort($arr){ //先判断是否需要继续进行 $length = count($arr); if($length $arr[$i]) { //放入左边数组 $left_array[] = $arr[$i]; } else { //放入右边 $right_array[] = $arr[$i]; } } //再分别对左边和右边的数组进行相同的排序处理方式递归调用这个函数 $left_array = quick_sort($left_array); $right_array = quick_sort($right_array); //合并 return array_merge($left_array, array($base_num), $right_array);; } "},"算法实现/一致性Hash.html":{"url":"算法实现/一致性Hash.html","title":"一致性Hash","keywords":"","body":"在使用分布式对数据进行存储时，经常会碰到需要新增节点来满足业务快速增长的需求。然而在新增节点时，如果处理不善会导致所有的数据重新分片，这对于某些系统来说可能是灾难性的。 那么是否有可行的方法，在数据重分片时，只需要迁移与之关联的节点而不需要迁移整个数据呢？当然有，在这种情况下我们可以使用一致性Hash来处理。 说明 用一定的哈希算法（哈希函数等）将一组服务器的多个（数目自己设定）节点随机映射分散到0-2的32次方之间，由于其随机分布，保证了其数据平均分布的特点； 用同一算法计算要存储数据的键，根据服务器节点确定其存储的服务器结点，由于每次用同一算法计算，所以得出的结果是相同的，使其查找定位准确； 查找数据时，再次用同一算法计算键，并查找服务器的数据结点； 从数据映射到的位置开始顺时针查找，将数据保存到找到的第一个服务器上。如果超过2的32次方仍然找不到服务器，就会保存到第一台memcached服务器上。 如果有一个服务器宕机，消除其服务器结点，并将数据放在下一个结点上，由于随机节点位置的随机性，所以数据被其他服务器平均负载，也就降低了宕机影响。 PHP实现 // 一致性哈希算法 class ConsistentHashing { protected $nodes = array(); //真实节点 protected $position = array(); //虚拟节点 protected $mul = 64; // 每个节点对应64个虚拟节点 /** * 把字符串转为32位符号整数 */ public function hash($str) { return sprintf('%u', crc32($str)); } /** * 核心功能 */ public function lookup($key) { $point = $this->hash($key); //先取圆环上最小的一个节点,当成结果 $node = current($this->position); // 循环获取相近的节点 foreach ($this->position as $key => $val) { if ($point position); //把数组的内部指针指向第一个元素，便于下次查询从头查找 return $node; } /** * 添加节点 */ public function addNode($node) { if(isset($this->nodes[$node])) return; // 添加节点和虚拟节点 for ($i = 0; $i mul; $i++) { $pos = $this->hash($node . '-' . $i); $this->position[$pos] = $node; $this->nodes[$node][] = $pos; } // 重新排序 $this->sortPos(); } /** * 删除节点 */ public function delNode($node) { if (!isset($this->nodes[$node])) return; // 循环删除虚拟节点 foreach ($this->nodes[$node] as $val) { unset($this->position[$val]); } // 删除节点 unset($this->nodes[$node]); } /** * 排序 */ public function sortPos() { ksort($this->position, SORT_REGULAR); } } // 测试 $con = new ConsistentHashing(); $con->addNode('a'); $con->addNode('b'); $con->addNode('c'); $con->addNode('d'); $key1 = 'www.zhihu.com'; $key2 = 'www.baidu.com'; $key3 = 'www.google.com'; $key4 = 'www.testabc.com'; echo 'key' . $key1 . '落在' . $con->lookup($key1) . '号节点上！'; echo 'key' . $key2 . '落在' . $con->lookup($key2) . '号节点上！'; echo 'key' . $key3 . '落在' . $con->lookup($key3) . '号节点上！'; echo 'key' . $key4 . '落在' . $con->lookup($key4) . '号节点上！'; "}}